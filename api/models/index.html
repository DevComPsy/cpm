<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>cpm.models - CPM library</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../assets/_mkdocstrings.css" rel="stylesheet" />
        <link href="../../style.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "cpm.models";
        var mkdocs_page_input_path = "api/models.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/python.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/bash.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/json.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/markdown.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/makefile.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> CPM library
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Getting Started</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../installation/">Installation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../troubleshooting/">Troubleshooting</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../roadmap/">Roadmap</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Examples</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/associative-learning/">Two associative learning models and blocking (cpm.models, cpm.generators)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/bandit-task/">Reinforcement learning with a two-armed bandit (cpm.models, cpm.generators, cpm.optimisation)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/metacognition/">Estimating metacognitive efficiency with meta-d (cpm.applications)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/model-parameter-recovery/">Parameter Recovery (cpm.generators, cpm.models, cpm.optimisation)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/model-recovery/">Model Recovery and Landscaping (cpm.models, cpm.generators, cpm.optimisation)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/fitting-hierarchical-estimation/">Estimating Empirical Priors (cpm.hierarchical)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/fitting-hierarchical/">Exploring hierarchical estimation of hyperparameters (cpm.hierarchical)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/hpc-example/">Scale to hypercomputing cluster</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">API Reference</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../generators/">cpm.generators</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">cpm.models</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#cpm.models.activation">activation</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.activation.CompetitiveGating">CompetitiveGating</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.activation.CompetitiveGating.compute">compute</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.activation.Offset">Offset</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.activation.Offset.compute">compute</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.activation.ProspectUtility">ProspectUtility</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.activation.ProspectUtility.compute">compute</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.activation.SigmoidActivation">SigmoidActivation</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.activation.SigmoidActivation.compute">compute</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#cpm.models.decision">decision</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.decision.ChoiceKernel">ChoiceKernel</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.decision.GreedyRule">GreedyRule</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.GreedyRule.choice">choice</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.GreedyRule.compute">compute</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.GreedyRule.config">config</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.decision.Sigmoid">Sigmoid</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.Sigmoid.choice">choice</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.Sigmoid.compute">compute</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.decision.Softmax">Softmax</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.Softmax.choice">choice</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.Softmax.compute">compute</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.Softmax.irreducible_noise">irreducible_noise</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#cpm.models.learning">learning</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.learning.DeltaRule">DeltaRule</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.DeltaRule.compute">compute</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.DeltaRule.noisy_learning_rule">noisy_learning_rule</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.DeltaRule.reset">reset</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.learning.HumbleTeacher">HumbleTeacher</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.HumbleTeacher.compute">compute</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.learning.KernelUpdate">KernelUpdate</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.KernelUpdate.compute">compute</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.KernelUpdate.config">config</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.learning.QLearningRule">QLearningRule</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.QLearningRule.compute">compute</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.learning.SeparableRule">SeparableRule</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.SeparableRule.compute">compute</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.SeparableRule.noisy_learning_rule">noisy_learning_rule</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.SeparableRule.reset">reset</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../optimisation/">cpm.optimisation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../hierarchical/">cpm.hierarchical</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../applications/">cpm.applications</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../utils/">cpm.utils</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">CPM library</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">API Reference</li>
      <li class="breadcrumb-item active">cpm.models</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="cpmmodels">cpm.models</h1>


<div class="doc doc-object doc-module">



<h2 id="cpm.models.activation" class="doc doc-heading">
            <code>cpm.models.activation</code>


</h2>

    <div class="doc doc-contents first">









  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="cpm.models.activation.CompetitiveGating" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CompetitiveGating</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">salience</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">P</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>A competitive attentional gating function, an attentional activation function, that incorporates stimulus salience in addition to the stimulus vector to modulate the weights.
It formalises the hypothesis that each stimulus has an underlying salience that competes to captures attentional focus (Paskewitz and Jones, 2020; Kruschke, 2001).</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>input</code></b>
                  (<code><span title="array_like">array_like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The input value. The stimulus representation (vector).</p>
              </div>
            </li>
            <li>
              <b><code>values</code></b>
                  (<code><span title="array_like">array_like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The values. A 2D array of values, where each row represents an outcome and each column represents a single stimulus.</p>
              </div>
            </li>
            <li>
              <b><code>salience</code></b>
                  (<code><span title="array_like">array_like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The salience value. A 1D array of salience values, where each value represents the salience of a single stimulus.</p>
              </div>
            </li>
            <li>
              <b><code>P</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The power value, also called attentional normalisation or brutality, which influences the degree of attentional competition.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">salience</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">att</span> <span class="o">=</span> <span class="n">CompetitiveGating</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">salience</span><span class="p">,</span> <span class="n">P</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">att</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([[0.03333333, 0.6       , 0.        ],</span>
<span class="go">       [0.2       , 0.13333333, 0.        ]])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>Kruschke, J. K. (2001). Toward a unified model of attention in associative learning. Journal of Mathematical Psychology, 45(6), 812-863.</p>
<p>Paskewitz, S., &amp; Jones, M. (2020). Dissecting exit. Journal of mathematical psychology, 97, 102371.</p>
</details>









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="cpm.models.activation.CompetitiveGating.compute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute the activations mediated by underlying salience.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="array_like">array_like</span></code>
              –
              <div class="doc-md-description">
                <p>The values updated with the attentional gain and stimulus vector.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.activation.Offset" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">Offset</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>A class for adding a scalar to one element of an input array.
In practice, this can be used to "shift" or "offset" the "value" of one particular stimulus, for example to represent a consistent bias for (or against) that stimulus.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>input</code></b>
                  (<code><span title="array_like">array_like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The input value. The stimulus representation (vector).</p>
              </div>
            </li>
            <li>
              <b><code>offset</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0</code>
)
              –
              <div class="doc-md-description">
                <p>The value to be added to one element of the input.</p>
              </div>
            </li>
            <li>
              <b><code>index</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>0</code>
)
              –
              <div class="doc-md-description">
                <p>The index of the element of the input vector to which the offset should be added.</p>
              </div>
            </li>
            <li>
              <b><code>**kwargs</code></b>
                  (<code><span title="dict">dict</span></code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Additional keyword arguments.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsetter</span> <span class="o">=</span> <span class="n">Offset</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">vals</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="mf">1.33</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsetter</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([3.43, 1.1])</span>
</code></pre></div>










  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="cpm.models.activation.Offset.compute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Add the offset to the requested input element.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="numpy.ndarray">ndarray</span></code>
              –
              <div class="doc-md-description">
                <p>The stimulus representation (vector) with offset added to the requested element.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.activation.ProspectUtility" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">ProspectUtility</span><span class="p">(</span><span class="n">magnitudes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">probabilities</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lambda_loss</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">utility_curve</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weighting</span><span class="o">=</span><span class="s1">&#39;tk&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>A class for computing choice utilities based on prospect theory.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>magnitudes</code></b>
                  (<code><span title="numpy.ndarray">ndarray</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A nested array where the outer dimension represents trials, with each trial
containing the potential outcome magnitudes for each option.</p>
              </div>
            </li>
            <li>
              <b><code>probabilities</code></b>
                  (<code><span title="numpy.ndarray">ndarray</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A nested array (with the same shape as magnitudes) where each entry contains
the probability of the corresponding outcome.</p>
              </div>
            </li>
            <li>
              <b><code>alpha</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The utility curvature parameter, used for both gains and losses.</p>
              </div>
            </li>
            <li>
              <b><code>beta</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>An optional parameter for the utility function used by Tversky and Kahneman (1992) for losses, defaults to <code>alpha</code> if not provided.</p>
              </div>
            </li>
            <li>
              <b><code>lambda_loss</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The loss aversion parameter (scaling losses relative to gains).</p>
              </div>
            </li>
            <li>
              <b><code>gamma</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The probability weighting curvature parameter (for gains with "tk" and both gains and losses with "power").</p>
              </div>
            </li>
            <li>
              <b><code>delta</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The attractiveness parameter, which determines the elevation of the weighting function in <code>prelec</code> and <code>gw</code> weighting functions. In <code>tk</code>, it is the probability weighting for losses. Defaults to <code>gamma</code> if not provided.</p>
              </div>
            </li>
            <li>
              <b><code>utility_curve</code></b>
                  (<code><span title="callable">callable</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>An optional utility function that takes the magnitude, alpha, and lambda_loss, and returns the utilities of each choice options. The default is a power utility function, see Notes.</p>
              </div>
            </li>
            <li>
              <b><code>weighting</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>&#39;tk&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>The definition of the weighting function. Should be one of 'tk', 'pd', or 'gw'. See Notes for details.</p>
              </div>
            </li>
            <li>
              <b><code>**kwargs</code></b>
                  (<code><span title="dict">dict</span></code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Additional keyword arguments.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="note" open>
  <summary>Notes</summary>
  <p>The different weighting functions currently implemented are:</p>
<pre><code>- `tk`: Tversky &amp; Kahneman (1992).
- `prelec`: Prelec (1998).
- `gw`: Gonzalez &amp; Wu (1999).
- `power` : Simple power function: w(p) = p^gamma
</code></pre>
<p>Following Tversky &amp; Kahneman (1992), the expected utility U of a choice option is defined as:</p>
<div class="arithmatex">\[
\mathcal{U} = \sum_{i=1}^{n} w(p_i) \cdot u(x_i)
\]</div>
<p>where <span class="arithmatex">\(w\)</span> is a weighting function of the probability p of a potential outcome,
and <span class="arithmatex">\(u\)</span> is the utility function of the magnitude x of a potential outcome.
The utility function <span class="arithmatex">\(u\)</span> is defined as a power function for both gains and losses. It is implemented
after Equation 5 in Tversky &amp; Kahneman (1992):</p>
<div class="arithmatex">\[
u(x) =
\begin{cases}
    x^\alpha &amp; \text{if } x \geq 0 \\
    -\lambda \cdot (-x)^\alpha &amp; \text{if } x &lt; 0
\end{cases}
\]</div>
<p>where <span class="arithmatex">\(\alpha\)</span> is the utility curvature parameter, and <span class="arithmatex">\(\lambda\)</span> is the loss aversion parameter.
The weighting function is implemented after Equation 6 in Tversky &amp; Kahneman (1992):</p>
<div class="arithmatex">\[
w(p) = \frac{p^\gamma}{(p^\gamma + (1 - p)^\gamma)^{1/\gamma}}
\]</div>
<p>where <code>gamma</code>, denoted via <span class="arithmatex">\(\gamma\)</span>, is the discriminability parameter of the weighting function.
In the original formulation of Tversky &amp; Kahneman (1992), losses are weighted with a different parameter,
<code>delta</code>, denoted via <span class="arithmatex">\(\delta\)</span>, that replaces <span class="arithmatex">\(\gamma\)</span> in the weighting function for losses.
In the current implementation, whether it is a gain or less is determined by the sign of the corresponding
magnitude.</p>
<p>Several other definitions of the weighting function have been proposed in the literature,
most notably in Prelec (1998) and Gonzalez &amp; Wu (1999).
Prelec (equation 3.2, 1998, pp. 503) proposed the following definition:</p>
<div class="arithmatex">\[
w(p) = \exp(-\delta \cdot (-\log(p))^\gamma)
\]</div>
<p>where <code>delta</code>, <span class="arithmatex">\(\delta\)</span>, and <code>gamma</code>, <span class="arithmatex">\(\gamma\)</span>, are the attractiveness and discriminability parameters of the weighting function.
Gonzalez &amp; Wu (equation 3, 1999, pp. 139) proposed the following definition:</p>
<div class="arithmatex">\[
w(p) = \frac{\delta \cdot p^\gamma}{\delta \cdot p^\gamma + (1 - p)^\gamma}
\]</div>
</details>

<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">cpm.models.activations</span><span class="w"> </span><span class="kn">import</span> <span class="n">ProspectUtility</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">magnitudes</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">probabilities</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ProspectUtility</span><span class="p">(</span>
<span class="go">        magnitudes=magnitudes,</span>
<span class="go">        probabilities=probabilities,</span>
<span class="go">        alpha=0.88,</span>
<span class="go">        lambda_loss=2.25,</span>
<span class="go">        gamma=0.61,</span>
<span class="go">        delta=1.0,</span>
<span class="go">        weighting=&quot;tk&quot;</span>
<span class="go">    )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected_utilities</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">expected_utilities</span><span class="p">)</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>Gonzalez, R., &amp; Wu, G. (1999). On the shape of the probability weighting function. Cognitive psychology, 38(1), 129-166.</p>
<p>Kahneman, D., &amp; Tversky, A. (1979). Prospect theory: An analysis of decision under risk. <em>Econometrica</em>, 47(2), 263–291.</p>
<p>Prelec, D. (1998). The probability weighting function. Econometrica, 497-527.</p>
<p>Tversky, A., &amp; Kahneman, D. (1992). Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and uncertainty, 5, 297-323.</p>
</details>









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="cpm.models.activation.ProspectUtility.compute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute the expected utility of each choice option.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="numpy.ndarray">ndarray</span></code>
              –
              <div class="doc-md-description">
                <p>The computed expected utility of each choice option.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.activation.SigmoidActivation" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">SigmoidActivation</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>Represents a sigmoid activation function.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>input</code></b>
                  (<code><span title="array_like">array_like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The input value. The stimulus representation (vector).</p>
              </div>
            </li>
            <li>
              <b><code>weights</code></b>
                  (<code><span title="array_like">array_like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The weights value. A 2D array of weights, where each row represents an outcome and each column represents a single stimulus.</p>
              </div>
            </li>
            <li>
              <b><code>**kwargs</code></b>
                  (<code><span title="dict">dict</span></code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Additional keyword arguments.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="cpm.models.activation.SigmoidActivation.compute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute the activation value using the sigmoid function.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="numpy.ndarray">ndarray</span></code>
              –
              <div class="doc-md-description">
                <p>The computed activation value.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="cpm.models.decision" class="doc doc-heading">
            <code>cpm.models.decision</code>


</h2>

    <div class="doc doc-contents first">









  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="cpm.models.decision.ChoiceKernel" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">ChoiceKernel</span><span class="p">(</span><span class="n">temperature_activations</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">temperature_kernel</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>A class representing a choice kernel based on a softmax function that incorporates the frequency of choosing an action.
It is based on Equation 7 in Wilson and Collins (2019).</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>temperature_activations</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.5</code>
)
              –
              <div class="doc-md-description">
                <p>The inverse temperature parameter for the softmax computation.</p>
              </div>
            </li>
            <li>
              <b><code>temperature_kernel</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.5</code>
)
              –
              <div class="doc-md-description">
                <p>The inverse temperature parameter for the kernel computation.</p>
              </div>
            </li>
            <li>
              <b><code>activations</code></b>
                  (<code><span title="ndarray">ndarray</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>An array of activations for the softmax function.</p>
              </div>
            </li>
            <li>
              <b><code>kernel</code></b>
                  (<code><span title="ndarray">ndarray</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>An array of kernel values for the softmax function.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="note" open>
  <summary>Notes</summary>
  <p>In order to get Equation 6 from Wilson and Collins (2019), either set <code>activations</code> to None (default) or set it to 0.</p>
</details>

<details class="see-also" open>
  <summary>See Also</summary>
  <p><a class="autorefs autorefs-internal" title="KernelUpdate(response, alpha, kernel, input, **kwargs)" href="#cpm.models.learning.KernelUpdate">cpm.models.learning.KernelUpdate</a>: A class representing a kernel update (Equation 5; Wilson and Collins, 2019) that updates the kernel values.</p>
</details>

<details class="references" open>
  <summary>References</summary>
  <p>Wilson, R. C., &amp; Collins, A. G. E. (2019). Ten simple rules for the computational modeling of behavioral data. eLife, 8, Article e49547.</p>
</details>

<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">activations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">choice_kernel</span> <span class="o">=</span> <span class="n">ChoiceKernel</span><span class="p">(</span><span class="n">temperature_activations</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">temperature_kernel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">choice_kernel</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([0.44028635, 0.55971365])</span>
</code></pre></div>










  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.decision.GreedyRule" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">GreedyRule</span><span class="p">(</span><span class="n">activations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>A class representing an ε-greedy rule based on Daw et al. (2006).</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>activations</code></b>
                  (<code><span title="ndarray">ndarray</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>An array of activations for the greedy rule.</p>
              </div>
            </li>
            <li>
              <b><code>epsilon</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0</code>
)
              –
              <div class="doc-md-description">
                <p>Exploration parameter. The probability of selecting a random action.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Attributes:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>activations</code></b>
                  (<code><span title="ndarray">ndarray</span></code>)
              –
              <div class="doc-md-description">
                <p>An array of activations for the greedy rule.</p>
              </div>
            </li>
            <li>
              <b><code>epsilon</code></b>
                  (<code><span title="float">float</span></code>)
              –
              <div class="doc-md-description">
                <p>Exploration parameter. The probability of selecting a random action.</p>
              </div>
            </li>
            <li>
              <b><code>policies</code></b>
                  (<code><span title="ndarray">ndarray</span></code>)
              –
              <div class="doc-md-description">
                <p>An array of outputs computed using the greedy rule.</p>
              </div>
            </li>
            <li>
              <b><code>shape</code></b>
                  (<code><span title="tuple">tuple</span></code>)
              –
              <div class="doc-md-description">
                <p>The shape of the activations array.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="references" open>
  <summary>References</summary>
  <p>Daw, N. D., O’Doherty, J. P., Dayan, P., Seymour, B., &amp; Dolan, R. J. (2006). Cortical substrates for exploratory decisions in humans. Nature, 441(7095), Article 7095. https://doi.org/10.1038/nature04766</p>
</details>









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="cpm.models.decision.GreedyRule.choice" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">choice</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Chooses the action based on the greedy rule.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>action</code></b>(                  <code><span title="int">int</span></code>
)              –
              <div class="doc-md-description">
                <p>The chosen action based on the greedy rule.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="cpm.models.decision.GreedyRule.compute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Computes the greedy rule.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>output</code></b>(                  <code><span title="ndarray">ndarray</span></code>
)              –
              <div class="doc-md-description">
                <p>A 2D array of outputs computed using the greedy rule.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="cpm.models.decision.GreedyRule.config" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">config</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Returns the configuration of the greedy rule.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>config</code></b>(                  <code><span title="dict">dict</span></code>
)              –
              <div class="doc-md-description">
                <p>A dictionary containing the configuration of the greedy rule.</p>
<ul>
<li>activations (ndarray): An array of activations for the greedy rule.</li>
<li>name (str): The name of the greedy rule.</li>
<li>type (str): The class of function it belongs.</li>
</ul>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.decision.Sigmoid" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">Sigmoid</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>A class representing a sigmoid function that takes an n by m array of activations and returns an n
array of outputs, where n is the number of output and m is the number of
inputs.</p>
<pre><code>The sigmoid function is defined as: 1 / (1 + e^(-temperature * (x - beta))).
</code></pre>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>temperature</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The inverse temperature parameter for the sigmoid function.</p>
              </div>
            </li>
            <li>
              <b><code>beta</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0</code>
)
              –
              <div class="doc-md-description">
                <p>It is the value of the output activation that results in an output rating
of P = 0.5.</p>
              </div>
            </li>
            <li>
              <b><code>activations</code></b>
                  (<code><span title="ndarray">ndarray</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>An array of activations for the sigmoid function.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">cpm.models.decision</span><span class="w"> </span><span class="kn">import</span> <span class="n">Sigmoid</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">temperature</span> <span class="o">=</span> <span class="mi">7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">activations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">Sigmoid</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigmoid</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([[0.66818777, 0.80218389]])</span>
</code></pre></div>










  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="cpm.models.decision.Sigmoid.choice" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">choice</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Chooses the action based on the sigmoid function.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>action</code></b>(                  <code><span title="int">int</span></code>
)              –
              <div class="doc-md-description">
                <p>The chosen action based on the sigmoid function.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="note" open>
  <summary>Notes</summary>
  <p>The choice is based on the probabilities of the sigmoid function, but it is not
guaranteed that the policy values will sum to 1. Therefore, the policies
are normalised to sum to 1 when generating a discrete choice.</p>
</details>

    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="cpm.models.decision.Sigmoid.compute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Computes the Sigmoid function.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>output</code></b>(                  <code><span title="ndarray">ndarray</span></code>
)              –
              <div class="doc-md-description">
                <p>A 2D array of outputs computed using the sigmoid function.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.decision.Softmax" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">Softmax</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>Softmax class for computing policies based on activations and temperature.</p>
<pre><code>The softmax function is defined as: e^(temperature * x) / sum(e^(temperature * x)).
</code></pre>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>temperature</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The inverse temperature parameter for the softmax computation.</p>
              </div>
            </li>
            <li>
              <b><code>xi</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The irreducible noise parameter for the softmax computation.</p>
              </div>
            </li>
            <li>
              <b><code>activations</code></b>
                  (<code><span title="numpy.ndarray">ndarray</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Array of activations for each possible outcome/action. It should be
a 2D ndarray, where each row represents an outcome and each column
represents a single stimulus.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="note" open>
  <summary>Notes</summary>
  <p>The inverse temperature parameter beta represents the degree of randomness in the choice process.
As beta approaches positive infinity, choices becomes more deterministic,
such that the choice option with the greatest activation is more likely to be chosen - it approximates a step function.
By contrast, as beta approaches zero, choices becomes random (i.e., the probabilities the choice options are approximately equal)
and therefore independent of the options' activations.</p>
<p><code>activations</code> must be a 2D array, where each row represents an outcome and each column represents a stimulus or other arbitrary features and variables.
If multiple values are provided for each outcome, the softmax function will sum these values up.</p>
<p>Note that if you have one value for each outcome (i.e. a classical bandit-like problem), and you represent it as a 1D
array, you must reshape it in the format specified for activations. So that if you have 3 stimuli
which all are actionable, <code>[0.1, 0.5, 0.22]</code>, you should have a 2D array of shape (3, 1), <code>[[0.1], [0.5], [0.22]]</code>.
You can see <a href="" title="./examples/examples2">Example 2</a> for a demonstration.</p>
</details>

<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">cpm.models.decision</span><span class="w"> </span><span class="kn">import</span> <span class="n">Softmax</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">temperature</span> <span class="o">=</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">activations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmax</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([0.30719589, 0.18632372, 0.50648039])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmax</span><span class="o">.</span><span class="n">choice</span><span class="p">()</span> <span class="c1"># This will randomly choose one of the actions based on the computed probabilities.</span>
<span class="go">2  </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Softmax</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">)</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([0.30719589, 0.18632372, 0.50648039])</span>
</code></pre></div>










  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="cpm.models.decision.Softmax.choice" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">choice</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Choose an action based on the computed policies.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>int</code></b>(                  <code>The chosen action based on the computed policies.</code>
)              –
              <div class="doc-md-description">
                
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="cpm.models.decision.Softmax.compute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute the policies based on the activations and temperature.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>numpy.ndarray: Array of computed policies.</code>
              –
              <div class="doc-md-description">
                
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="cpm.models.decision.Softmax.irreducible_noise" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">irreducible_noise</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Extended softmax class for computing policies based on activations, with parameters inverse temperature and irreducible noise.</p>
<p>The softmax function with irreducible noise is defined as:</p>
<pre><code>(e^(beta * x) / sum(e^(beta * x))) * (1 - xi) + (xi / length(x)),
</code></pre>
<p>where x is the input array of activations, beta is the inverse temperature parameter, and xi is the irreducible noise parameter.</p>


<details class="note" open>
  <summary>Notes</summary>
  <p>The irreducible noise parameter xi accounts for attentional lapses in the choice process.
Specifically, the terms (1-xi) + (xi/length(x)) cause the choice probabilities to be proportionally scaled towards 1/length(x).
Relatively speaking, this increases the probability that an option is selected if its activation is exceptionally low.
This may seem counterintuitive in theory, but in practice it enables the model to capture highly surprising responses that can occur during attentional lapses.</p>
</details>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>numpy.ndarray: Array of computed policies with irreducible noise.</code>
              –
              <div class="doc-md-description">
                
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">activations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">noisy_softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">noisy_softmax</span><span class="o">.</span><span class="n">irreducible_noise</span><span class="p">()</span>
<span class="go">array([0.4101454, 0.5898546])</span>
</code></pre></div>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="cpm.models.learning" class="doc doc-heading">
            <code>cpm.models.learning</code>


</h2>

    <div class="doc doc-contents first">









  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="cpm.models.learning.DeltaRule" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">DeltaRule</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zeta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>DeltaRule class computes the prediction error for a given input and target value.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>alpha</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The learning rate.</p>
              </div>
            </li>
            <li>
              <b><code>zeta</code></b>
              –
              <div class="doc-md-description">
                <p>The constant fraction of the magnitude of the prediction error.</p>
              </div>
            </li>
            <li>
              <b><code>weights</code></b>
                  (<code><span title="array">array</span> - <span title="like">like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The value matrix, where rows are outcomes and columns are stimuli or features. The values can be anything; for example belief values, association weights, connection weights, Q-values.</p>
              </div>
            </li>
            <li>
              <b><code>feedback</code></b>
                  (<code><span title="array">array</span> - <span title="like">like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The target values or feedback, sometimes referred to as teaching signals. These are the values that the algorithm should learn to predict.</p>
              </div>
            </li>
            <li>
              <b><code>input</code></b>
                  (<code><span title="array">array</span> - <span title="like">like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The input value. The stimulus representation in the form of a 1D array, where each element can take a value of 0 and 1.</p>
              </div>
            </li>
            <li>
              <b><code>**kwargs</code></b>
                  (<code><span title="dict">dict</span></code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Additional keyword arguments.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="see-also" open>
  <summary>See Also</summary>
  <p><a class="autorefs autorefs-internal" title="SeparableRule(alpha=None, zeta=None, weights=None, feedback=None, input=None, **kwargs)" href="#cpm.models.learning.SeparableRule">cpm.models.learning.SeparableRule</a> : A class representing a learning rule based on the separable error-term of Bush and Mosteller (1951).</p>
</details>

<details class="note" open>
  <summary>Notes</summary>
  <p>The delta-rule is a summed error term, which means that the error is defined as
the difference between the target value and the summed activation of all values
for a given output units target value available on the current trial/state. For separable
error term, see the Bush and Mosteller (1951) rule.</p>
<p>The current implementation is based on the Gluck and Bower's (1988) delta rule, an
extension of the Rescorla and Wagner (1972) learning rule to multi-outcome learning. Such that</p>
<div class="arithmatex">\[
\Delta w_{ij} = \alpha \cdot (\lambda_i - \sum_j w_{ij}) \cdot x_j
\]</div>
<p>where <span class="arithmatex">\(\Delta w_{ij}\)</span> is the change in weight for the <span class="arithmatex">\(j\)</span>-th stimulus for the <span class="arithmatex">\(i\)</span>-th outcome,
<span class="arithmatex">\(\lambda_i\)</span> is the target (feedback) value for the i-th outcome, <span class="arithmatex">\(w_ij\)</span> is the weights of stimulus <span class="arithmatex">\(j\)</span> 
for the <span class="arithmatex">\(i\)</span>-th outcome,
<span class="arithmatex">\(x_j\)</span> is the j-th stimulus input, and <span class="arithmatex">\(\alpha\)</span> is the learning rate. This is consistent with the
Rescorla and Wagner (1972)'s learning rule incorporating the summed error term. </p>
</details>

<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">cpm.models.learning</span><span class="w"> </span><span class="kn">import</span> <span class="n">DeltaRule</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">teacher</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">delta_rule</span> <span class="o">=</span> <span class="n">DeltaRule</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">zeta</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="n">teacher</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">delta_rule</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([[ 0.07,  0.07,  0.  ],</span>
<span class="go">       [-0.09, -0.09, -0.  ]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">delta_rule</span><span class="o">.</span><span class="n">noisy_learning_rule</span><span class="p">()</span>
<span class="go">array([[ 0.05755793,  0.09214091,  0.],</span>
<span class="go">       [-0.08837513, -0.1304325 ,  0.]])</span>
</code></pre></div>
    <p>This implementation generalises to n-dimensional matrices, which means
that it can be applied to both single- and multi-outcome learning paradigms.</p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">teacher</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">delta_rule</span> <span class="o">=</span> <span class="n">DeltaRule</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="n">teacher</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">delta_rule</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([[0.03, 0.03, 0.  , 0.  ]])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>Gluck, M. A., &amp; Bower, G. H. (1988). From conditioning to category learning: An adaptive network model. Journal of Experimental Psychology: General, 117(3), 227–247.</p>
<p>Rescorla, R. A., &amp; Wagner, A. R. (1972). A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. In A. H. Black &amp; W. F. Prokasy (Eds.), Classical conditioning II: Current research and theory (pp. 64-99). New York:Appleton-Century-Crofts.</p>
<p>Widrow, B., &amp; Hoff, M. E. (1960, August). Adaptive switching circuits. In IRE WESCON convention record (Vol. 4, No. 1, pp. 96-104).</p>
</details>









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="cpm.models.learning.DeltaRule.compute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute the prediction error using the delta learning rule. It is based on the
Gluck and Bower's (1988) delta rule, an extension to Rescorla and Wagner
(1972), which was identical to that of Widrow and Hoff (1960).</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="ndarray">ndarray</span></code>
              –
              <div class="doc-md-description">
                <p>The prediction error for each stimuli-outcome mapping with learning noise.
It has the same shape as the weights input argument.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="cpm.models.learning.DeltaRule.noisy_learning_rule" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">noisy_learning_rule</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Add random noise to the prediction error computed from the delta learning rule as specified
Findling et al. (2019). It is inspired by Weber's law of intensity
sensation.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="ndarray">ndarray</span></code>
              –
              <div class="doc-md-description">
                <p>The prediction error for each stimuli-outcome mapping with learning noise.
It has the same shape as the weights input argument.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="references" open>
  <summary>References</summary>
  <p>Findling, C., Skvortsova, V., Dromnelle, R., Palminteri, S., and Wyart, V. (2019). Computational noise in reward-guided learning drives behavioral variability in volatile environments. Nature Neuroscience 22, 2066–2077</p>
</details>

    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="cpm.models.learning.DeltaRule.reset" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">reset</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Reset the weights to zero.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.learning.HumbleTeacher" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">HumbleTeacher</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>A humbe teacher learning rule (Kruschke, 1992; Love, Gureckis, and Medin, 2004) for multi-dimensional outcome learning.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Attributes:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>alpha</code></b>
                  (<code><span title="float">float</span></code>)
              –
              <div class="doc-md-description">
                <p>The learning rate.</p>
              </div>
            </li>
            <li>
              <b><code>input</code></b>
                  (<code><span title="ndarray">ndarray</span> or <span title="array_like">array_like</span></code>)
              –
              <div class="doc-md-description">
                <p>The input value. The stimulus representation in the form of a 1D array, where each element can take a value of 0 and 1.</p>
              </div>
            </li>
            <li>
              <b><code>weights</code></b>
                  (<code><span title="ndarray">ndarray</span></code>)
              –
              <div class="doc-md-description">
                <p>The weights value. A 2D array of weights, where each row represents an outcome and each column represents a single stimulus.</p>
              </div>
            </li>
            <li>
              <b><code>teacher</code></b>
                  (<code><span title="ndarray">ndarray</span></code>)
              –
              <div class="doc-md-description">
                <p>The target values or feedback, sometimes referred to as teaching signals. These are the values that the algorithm should learn to predict.</p>
              </div>
            </li>
            <li>
              <b><code>shape</code></b>
                  (<code><span title="tuple">tuple</span></code>)
              –
              <div class="doc-md-description">
                <p>The shape of the weight matrix.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>alpha</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The learning rate.</p>
              </div>
            </li>
            <li>
              <b><code>weights</code></b>
                  (<code><span title="array">array</span> - <span title="like">like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The input value. The stimulus representation in the form of a 1D array, where each element can take a value of 0 and 1.</p>
              </div>
            </li>
            <li>
              <b><code>feedback</code></b>
                  (<code><span title="array">array</span> - <span title="like">like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The target values or feedback, sometimes referred to as teaching signals. These are the values that the algorithm should learn to predict.</p>
              </div>
            </li>
            <li>
              <b><code>input</code></b>
                  (<code><span title="array">array</span> - <span title="like">like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The input value. The stimulus representation in the form of a 1D array, where each element can take a value of 0 and 1.</p>
              </div>
            </li>
            <li>
              <b><code>**kwargs</code></b>
                  (<code><span title="dict">dict</span></code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Additional keyword arguments.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="note" open>
  <summary>Notes</summary>
  <p>The humble teacher is a learning rule that is based on the idea that if output node activations are larger than the teaching signal, they should not be counted as error, but should be rewarded. It is defined as:</p>
<div class="arithmatex">\[
t_k = \begin{cases}
\min(-1, a_k) &amp; \text{if } t_k = 0 \text{ if stimulus is not followed by outcome/category-label} \\
\max(1, a_k) &amp; \text{if } t_k = 1 \text{ if stimulus is followed by outcome/category-label}
\end{cases}
\]</div>
<p>where <span class="arithmatex">\(t_k\)</span> is the teaching signal. Then the change in weights is computed according to the delta rule (Rescorla &amp; Wagner, 1972; Rumelhart, Hinton &amp; Williams, 1986; Gluck &amp; Bower, 1988):</p>
<div class="arithmatex">\[
\Delta w_{ij} = \alpha \cdot (t_k - a_k) \cdot x_j
\]</div>
<p>where <span class="arithmatex">\(\Delta w_{ij}\)</span> is the change in weight for the <span class="arithmatex">\(j\)</span>-th stimulus for the <span class="arithmatex">\(i\)</span>-th outcome, <span class="arithmatex">\(t_k\)</span> is the teaching signal for the <span class="arithmatex">\(k\)</span>-th outcome, <span class="arithmatex">\(a_k\)</span> is the summed activation of all nodes connected to the <span class="arithmatex">\(k\)</span>-th outcome, <span class="arithmatex">\(x_j\)</span> is the j-th stimulus input, and <span class="arithmatex">\(\alpha\)</span> is the learning rate.</p>
</details>

<details class="references" open>
  <summary>References</summary>
  <p>Gluck, M. A., &amp; Bower, G. H. (1988). From conditioning to category learning: An adaptive network model. Journal of Experimental Psychology: General, 117(3), 227–247.</p>
<p>Kruschke, J. K. (1992). ALCOVE: An exemplar-based connectionist model of category learning. Psychological Review, 99, 22–44.</p>
<p>Rescorla, R. A., &amp; Wagner, A. R. (1972). A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. In A. H. Black &amp; W. F. Prokasy (Eds.), Classical conditioning II: Current research and theory (pp. 64-99). New York:Appleton-Century-Crofts.</p>
<p>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. nature, 323(6088), 533-536.</p>
</details>

<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">cpm.models.learning</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumbleTeacher</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">teacher</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">humble_teacher</span> <span class="o">=</span> <span class="n">HumbleTeacher</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="n">teacher</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">humble_teacher</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([[-0.06,  0.04,  0.14],</span>
<span class="go">    [ 0.4 ,  0.5 ,  0.6 ]])</span>
</code></pre></div>










  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="cpm.models.learning.HumbleTeacher.compute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute the weights using the CPM learning rule.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>weights</code></b>(                  <code><span title="numpy.ndarray">ndarray</span></code>
)              –
              <div class="doc-md-description">
                <p>The updated weights matrix.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.learning.KernelUpdate" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">KernelUpdate</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>A class representing a learning rule for updating the choice kernel as specified by Equation 5 in Wilson and Collins (2019).</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>response</code></b>
                  (<code><span title="ndarray">ndarray</span></code>)
              –
              <div class="doc-md-description">
                <p>The response vector. It must be a binary numpy.ndarray, so that each element corresponds to a response option. If there are 4 response options, and the second was selected, it would be represented as <code>[0, 1, 0, 0]</code>.</p>
              </div>
            </li>
            <li>
              <b><code>alpha</code></b>
                  (<code><span title="float">float</span></code>)
              –
              <div class="doc-md-description">
                <p>The kernel learning rate.</p>
              </div>
            </li>
            <li>
              <b><code>kernel</code></b>
                  (<code><span title="ndarray">ndarray</span></code>)
              –
              <div class="doc-md-description">
                <p>The kernel used for learning. It is a 1D array of kernel values, where each element corresponds to a response option. Each element must correspond to the same response option in the <code>response</code> vector.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="note" open>
  <summary>Notes</summary>
  <p>The kernel update component is used to represent how likely a given response is to be chosen based on the frequency it was chosen in the past.
This can then be integrated into a choice kernel decision policy.</p>
</details>

<details class="see-also" open>
  <summary>See Also</summary>
  <p><a class="autorefs autorefs-internal" title="ChoiceKernel(temperature_activations=0.5, temperature_kernel=0.5, activations=None, kernel=None, **kwargs)" href="#cpm.models.decision.ChoiceKernel">cpm.models.decision.ChoiceKernel</a> : A class representing a choice kernel decision policy.</p>
</details>

<details class="references" open>
  <summary>References</summary>
  <p>Wilson, Robert C., and Anne GE Collins. Ten simple rules for the computational modeling of behavioral data. Elife 8 (2019): e49547.</p>
</details>









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="cpm.models.learning.KernelUpdate.compute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute the change in the kernel based on the given response, rate, and kernel, and return the updated kernel.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>output</code></b>(                  <code>numpy.ndarray:</code>
)              –
              <div class="doc-md-description">
                <p>The computed change of the kernel.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="cpm.models.learning.KernelUpdate.config" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">config</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Get the configuration of the kernel update component.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>config</code></b>(                  <code><span title="dict">dict</span></code>
)              –
              <div class="doc-md-description">
                <p>A dictionary containing the configuration parameters of the kernel update component.</p>
<ul>
<li>response (float): The response of the system.</li>
<li>rate (float): The learning rate.</li>
<li>kernel (list): The kernel used for learning.</li>
<li>input (str): The name of the input.</li>
<li>name (str): The name of the kernel update component class.</li>
<li>type (str): The type of the kernel update component.</li>
</ul>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.learning.QLearningRule" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">QLearningRule</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reward</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>Q-learning rule (Watkins, 1989) for a one-dimensional array of Q-values.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>alpha</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.5</code>
)
              –
              <div class="doc-md-description">
                <p>The learning rate. Default is 0.5.</p>
              </div>
            </li>
            <li>
              <b><code>gamma</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The discount factor. Default is 0.1.</p>
              </div>
            </li>
            <li>
              <b><code>values</code></b>
                  (<code><span title="ndarray">ndarray</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The values matrix.  It is a 1D array of Q-values active for the current state, where each element corresponds to an action.</p>
              </div>
            </li>
            <li>
              <b><code>reward</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The reward received on the current state.</p>
              </div>
            </li>
            <li>
              <b><code>maximum</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The maximum estimated reward for the next state.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="note" open>
  <summary>Notes</summary>
  <p>The Q-learning rule is a model-free reinforcement learning algorithm that is used to learn the value of an action in a given state.
It is defined as</p>
<pre><code>Q(s, a) = Q(s, a) + alpha * (r + gamma * max(Q(s', a')) - Q(s, a)),
</code></pre>
<p>where <code>Q(s, a)</code> is the value of action <code>a</code> in state <code>s</code>, <code>r</code> is the reward received on the current state, <code>gamma</code> is the discount factor, and <code>max(Q(s', a'))</code> is the maximum estimated reward for the next state.</p>
</details>

<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">cpm.models.learning</span><span class="w"> </span><span class="kn">import</span> <span class="n">QLearningRule</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">component</span> <span class="o">=</span> <span class="n">QLearningRule</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">,</span> <span class="n">reward</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">component</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([1.8  , 1.35 , 1.791])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>Watkins, C. J. C. H. (1989). Learning from delayed rewards.</p>
<p>Watkins, C. J., &amp; Dayan, P. (1992). Q-learning. Machine learning, 8, 279-292.</p>
</details>









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="cpm.models.learning.QLearningRule.compute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute the change in values based on the given values, reward, and parameters, and return the updated values.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>output</code></b>(                  <code>numpy.ndarray:</code>
)              –
              <div class="doc-md-description">
                <p>The computed output values.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.learning.SeparableRule" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">SeparableRule</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zeta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>A class representing a learning rule based on the separable error-term of
Bush and Mosteller (1951).</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>alpha</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The learning rate.</p>
              </div>
            </li>
            <li>
              <b><code>zeta</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The constant fraction of the magnitude of the prediction error, also called Weber's scaling.</p>
              </div>
            </li>
            <li>
              <b><code>weights</code></b>
                  (<code><span title="array">array</span> - <span title="like">like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The value matrix, where rows are outcomes and columns are stimuli or features. The values can be anything; for example belief values, association weights, connection weights, Q-values.</p>
              </div>
            </li>
            <li>
              <b><code>feedback</code></b>
                  (<code><span title="array">array</span> - <span title="like">like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The target values or feedback, sometimes referred to as teaching signals. These are the values that the algorithm should learn to predict.</p>
              </div>
            </li>
            <li>
              <b><code>input</code></b>
                  (<code><span title="array">array</span> - <span title="like">like</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The input value. The stimulus representation in the form of a 1D array, where each element can take a value of 0 and 1.</p>
              </div>
            </li>
            <li>
              <b><code>**kwargs</code></b>
                  (<code><span title="dict">dict</span></code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Additional keyword arguments.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="see-also" open>
  <summary>See Also</summary>
  <p><a class="autorefs autorefs-internal" title="DeltaRule(alpha=None, zeta=None, weights=None, feedback=None, input=None, **kwargs)" href="#cpm.models.learning.DeltaRule">cpm.models.learning.DeltaRule</a> : An extension of the Rescorla and Wagner (1972) learning rule by Gluck and Bower (1988) to allow multi-outcome learning.</p>
</details>

<details class="note" open>
  <summary>Notes</summary>
  <p>This type of learning rule was among the earliest formal models of associative learning (Le Pelley, 2004), which were based on standard linear operators (Bush &amp; Mosteller, 1951; Estes, 1950; Kendler, 1971). It is used in a variety of reinforcement learning models. This learning rule is defined in <code>cpm</code> as</p>
<div class="arithmatex">\[
\Delta w_{ij} = \alpha \cdot (\lambda_i - w_{ij}) \cdot x_j
\]</div>
<p>which is consistent with the modification of the Rescorla and Wagner (1972) learning rule by Sutton and Barto (2018). The current implementation generalises to any number of outcomes and stimuli, which means that it can be applied to both single- and multi-outcome learning paradigms. </p>
</details>

<details class="references" open>
  <summary>References</summary>
  <p>Bush, R. R., &amp; Mosteller, F. (1951). A mathematical model for simple learning. Psychological Review, 58, 313–323</p>
<p>Estes, W. K. (1950). Toward a statistical theory of learning. Psychological Review, 57, 94–107</p>
<p>Kendler, T. S. (1971). Continuity theory and cue dominance. In J. T. Spence (Ed.), Essays in neobehaviorism: A memorial volume to Kenneth W. Spence. New York: Appleton-Century-Crofts.</p>
<p>Le Pelley, M. E. (2004). The role of associative history in models of associative learning: A selective review and a hybrid model. Quarterly Journal of Experimental Psychology Section B, 57(3), 193-243.</p>
</details>









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="cpm.models.learning.SeparableRule.compute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Computes the prediction error using the learning rule.</p>


<details class="returns:" open>
  <summary>Returns:</summary>
  <p>ndarray
    The prediction error for each stimuli-outcome mapping.
    It has the same shape as the weights input argument.</p>
</details>

    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="cpm.models.learning.SeparableRule.noisy_learning_rule" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">noisy_learning_rule</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Add random noise to the prediction error computed from the delta learning rule as specified
Findling et al. (2019). It is inspired by Weber's law of intensity
sensation.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="ndarray">ndarray</span></code>
              –
              <div class="doc-md-description">
                <p>The prediction error for each stimuli-outcome mapping with learning noise.
It has the same shape as the weights input argument.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="references" open>
  <summary>References</summary>
  <p>Findling, C., Skvortsova, V., Dromnelle, R., Palminteri, S., and Wyart, V. (2019). Computational noise in reward-guided learning drives behavioral variability in volatile environments. Nature Neuroscience 22, 2066–2077</p>
</details>

    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="cpm.models.learning.SeparableRule.reset" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">reset</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Resets the weights to zero.</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../generators/" class="btn btn-neutral float-left" title="cpm.generators"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../optimisation/" class="btn btn-neutral float-right" title="cpm.optimisation">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../generators/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../optimisation/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
