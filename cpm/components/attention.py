import numpy as np


# class AttentionGateLearning:
#     """
#     A learning rule to update attentional values (salience of stimuli) as a function of gradient descent on error.
#     """
#     def __init__(self, theta, weights, input, teacher, salience, *args, **kwargs):
#         self.theta = theta
#         self.weights = weights
#         self.input = input
#         self.teacher = teacher
#         self.salience = salience


# class MackintoshUpdate:
#     """
#     A learning rule to update attentional values (salience of stimuli) in the spirit of Mackintosh (1975) as a function of the prediction error.
#     The current implementation follows the Equation X from Le Pelley et al. (200X).
#     """

#     def __init__(self) -> None:
#         pass
