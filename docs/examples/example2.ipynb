{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Reinforcement learning with a two-armed bandit.\n",
    "\n",
    "This is an **intermediate** level tutorial, where we assume that you are familiar with the basics of reinforcement learning and the two-armed bandit problem.\n",
    "In this example, we will apply and fit a reinforcement learning model to a two-armed bandit problem.\n",
    "The model will be consist of a $\\epsilon$-greedy policy and a prediction error term.\n",
    "\n",
    "In the following tutorial, we will use `prettyformatter` to print some nice and organised output in the Jupyter Notebook.\n",
    "In your python script, you do not need to use `prettyformatter`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-armed bandits\n",
    "\n",
    "Two-armed bandit is a two-alternative forced-choice reinforcement learning problem, part of the bigger set of multi-armed bandit problems. \n",
    "In these problems, the person is faced with multiple choices, each with a different degree of reward.\n",
    "The goal of the person is to learn which choice is the best and to maximize the reward over time.\n",
    "In this example, we will consider a two-armed bandit problem, where the person is faced with two choices (select an item on the left or the item on the right), each with a different reward.\n",
    "There are 4 different items that can appear in combinations of two, and the reward for each item varies.\n",
    "For example, if item 1 has a chance of 0.7 of giving a reward, then you can expect to receive a reward 70% of the time when you select item 1.\n",
    "The problem is that the underlying structure of the item-reward mapping is unknown.\n",
    "Here we will use the following item-reward mapping:\n",
    "\n",
    "| Item | Reward |\n",
    "|------|--------|\n",
    "| 1    | 0.8    |\n",
    "| 2    | 0.2    |\n",
    "| 3    | 0.5    |\n",
    "| 4    | 0.9    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "\n",
    "First, we will import the data and get it ready for the toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stim_left</th>\n",
       "      <th>stim_right</th>\n",
       "      <th>reward_left</th>\n",
       "      <th>reward_right</th>\n",
       "      <th>ppt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stim_left  stim_right  reward_left  reward_right  ppt\n",
       "0          4           2          0.0           1.0    1\n",
       "1          2           4          1.0           0.0    1\n",
       "2          2           3          1.0           1.0    1\n",
       "3          4           1          0.0           1.0    1\n",
       "4          4           2          0.0           1.0    1\n",
       "5          4           3          0.0           1.0    1\n",
       "6          2           1          1.0           1.0    1\n",
       "7          3           2          1.0           1.0    1\n",
       "8          3           2          1.0           1.0    1\n",
       "9          3           1          1.0           1.0    1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettyformatter import pprint\n",
    "\n",
    "experiment = pd.read_csv('bandit.csv', header=0)\n",
    "experiment.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at what each column represents:\n",
    "\n",
    "- `left`: the stimulus presented on the left side.\n",
    "- `right`: the stimulus presented on the right side.\n",
    "- `reward_left`: the reward received when the left stimulus is selected.\n",
    "- `reward_right`: the reward received when the right stimulus is selected.\n",
    "- `ppt`: the participant number.\n",
    "\n",
    "Notice that for now, we don't have any actual data recorded from participants.\n",
    "That is because, for now, we will only import the environment, containing all states and rewards.\n",
    "So, let us convert the data into a format that the toolbox can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, the toolbox provides a function to convert the data into the required format.\n",
    "We will use the `pandas_to_dict` function available in the `cpm.utils` module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants: 100\n"
     ]
    }
   ],
   "source": [
    "from cpm.utils import pandas_to_dict\n",
    "\n",
    "experiment = pandas_to_dict(experiment, participant=\"ppt\", stimuli='stim', feedback='reward')\n",
    "length = len(experiment)\n",
    "print(f\"Number of participants: {length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have a list of dictionaries, where each dictionary represents an experimental session that a participant might complete.\n",
    "If you have 100 participants or sessions, then you will have 100 dictionaries in the list -each with their unique trial order (schedule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key variables in the dictionary: dict_keys(['trials', 'feedback', 'ppt'])\n",
      "Number of trials and number of the stimuli on each of those trials: (60, 2)\n",
      "Number of trials and number of the feedback on each of those trials: (60, 2)\n",
      "All looks good! We are ready to go!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Key variables in the dictionary: {experiment[0].keys()}\")\n",
    "print(f\"Number of trials and number of the stimuli on each of those trials: {experiment[0].get('trials').shape}\")\n",
    "print(f\"Number of trials and number of the feedback on each of those trials: {experiment[0].get('feedback').shape}\")\n",
    "print(\"All looks good! We are ready to go!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us see what each value within the dictionary represents:\n",
    "\n",
    "- `trials`: the stimuli presented in the session. It must be a 2D numpy.ndarray (a matrix). Each row represents a trial, and the columns represent the left and right stimuli presented in each trial.\n",
    "- `feedback`: the rewards that could be obtained from the stimuli. It must be a 2D numpy.ndarray (a matrix). Each row represents a trial, and the columns represent the reward that could be obtained from the corresponding stimulus in the `stimuli` key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "Let us quickly go through the model we will use.\n",
    "\n",
    "Each stimulus has an associated value, which is the expected reward that can be obtained from selecting that stimulus.\n",
    "\n",
    "Let $Q(a)$ be the estimated value of action $a$.\n",
    "On each trial, $t$, there are two stimuli present, so that $Q(a)$ could be $Q(\\text{left})$ or $Q(\\text{right})$, where the corresponding Q-values are derived from the associated value of the stimuli present on left or right.\n",
    "More formally, we can say that the expected value of the action $a$ selected at time $t$ is given by:\n",
    "\n",
    "$$\n",
    "Q_t(a) = \\mathbb{E}[R_t | A_t = a]\n",
    "$$\n",
    "\n",
    "where $R_t$ is the reward received at time $t$, and $A_t$ is the action selected at time $t$.\n",
    "\n",
    "On each trial $t$, the $\\epsilon$-greedy policy selects a random action with probability $\\epsilon$, the exploration rate parameter, and selects the action with the highest estimated value with probability $1-n\\epsilon$, where $n$ the number of possible actions.\n",
    "So, on each trial, the model will select an action (left or right) based on the following policy:\n",
    "\n",
    "$$\n",
    "A_t = \n",
    "\\begin{cases} \n",
    "\\text{random action} & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max_a Q_t(a) & \\text{with probability } 1 - n \\epsilon \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $A_t$ is the action selected at time $t$, and $Q_t(a)$ is the estimated value of action $a$ at time $t$.\n",
    "\n",
    "The model will calculate the prediction error according to the following learning rule:\n",
    "\n",
    "$$\n",
    "\\Delta Q_t(A_t) = \\alpha \\times \\Big[ R_t - Q_t(A_t) \\Big]\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, and $R_t$ is the reward received at time $t$.\n",
    "Note that this rule is often reported as the Rescorla-Wagner learning rule, where for each action/outcome, the prediction error is the difference between the reward received and the sum of all values present on each trial for that action.\n",
    "In our case, we only have one value for each action, so the Rescorla-Wagner learning rule is reduced to the Bush and Mosteller separable error term.\n",
    "Q-values are then updated as follows:\n",
    "\n",
    "$$\n",
    "Q_{t+1}(A_t) = Q_t(A_t) + \\Delta Q_t(A_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "In order to use the toolbox, you will have to specify **the computations for one single trial**.\n",
    "Rest assured, you do not have to build the model from scratch.\n",
    "We have fully-fledged models in `cpm.applications` that you can use, but we also have all the building blocks in `cpm.components` that you can use to build your own model.\n",
    "\n",
    "For now, let us simplify the problem and start by specifying what information we need on each trial.\n",
    "Here, we are only concerned about the environment, and not the participant's choices or model parameters.\n",
    "This information will usually be extracted from the data we just imported.\n",
    "Here, we create this to help us develop the model.\n",
    "\n",
    "Here, we need to specify the following:\n",
    "\n",
    "- `trials`: the stimuli presented in the trial. It must be a 1D numpy.ndarray (a vector) with two elements, representing the left and right stimuli presented in the trial. For example, if stimuli 1 and 4 was presented, then the vector would be [1, 4].\n",
    "- `feedback`: the rewards that could be obtained from selecting the stimuli - essentially the feedback about the choice the model makes. It must be a 1D numpy.ndarray (a vector) with two elements, representing the reward that could be obtained from the left and right stimuli presented in the trial. For example, if the outcome for selecting the left stimulus was 1 and the outcome for selecting the right stimulus was 0, then the vector would be [1, 0].\n",
    "\n",
    "You can specify as many other variables as you need, but these are the minimum required to build the model.\n",
    "All variables specified here must also be `array_like` objects (e.g., numpy arrays, lists, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = {\n",
    "    \"trials\": [1, 4],\n",
    "    \"feedback\": [1, 0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we build the model, we also have to talk about the `cpm.Parameter` class.\n",
    "This class is used to specify the parameters of the model, including various bounds and priors.\n",
    "Let us specify the parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The learning rate of the model: {'value': 0.12, 'prior': 'normal', 'lower': 0, 'upper': 1}\n",
      "The exploration rate of the model: {'value': 0.1, 'prior': 'normal', 'lower': 0, 'upper': 1}\n",
      "The initial value of each action: {'value': array([0.25, 0.25, 0.25, 0.25]), 'prior': 'normal', 'lower': 0, 'upper': 1}\n"
     ]
    }
   ],
   "source": [
    "from cpm.generators import Parameters\n",
    "\n",
    "parameters = Parameters(alpha = 0.12, epsilon = 0.1, values = np.array([0.25, 0.25, 0.25, 0.25]))\n",
    "print(f\"The learning rate of the model: {parameters.alpha.export()}\")\n",
    "print(f\"The exploration rate of the model: {parameters.epsilon.export()}\")\n",
    "print(f\"The initial value of each action: {parameters.values.export()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can immediately see that the toolbox defined a bunch of things for us.\n",
    "This includes the prior and the parameter ranges as well.\n",
    "One thing we have to clarify is the value for each action.\n",
    "Here we initialised the value of each action to 0.25.\n",
    "You can initialise it to 0 as well, which is a common practice.\n",
    "If you use a rating scale in your experiment and use a Logistic function for your choice rule, the number you initialise the value will make a difference.\n",
    "The way you represent the values will also depend on the model building blocks you use and how you specify the computations for each trial.\n",
    "Nonetheless, even though we treated it as a parameter, we do not need to estimate it later on.\n",
    "\n",
    "That was enough preparation, let us build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy': array([0.5, 0.5]),\n",
       " 'response': 1,\n",
       " 'reward': 0,\n",
       " 'values': array([0.25, 0.25, 0.25, 0.22]),\n",
       " 'change': array([[-0.  , -0.  , -0.  , -0.03]]),\n",
       " 'activation': array([0.25, 0.25]),\n",
       " 'dependent': array([1])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cpm.models import learning, decision, utils\n",
    "import copy\n",
    "\n",
    "def model(parameters, trial):\n",
    "    # pull out the parameters\n",
    "    alpha = parameters.alpha\n",
    "    epsilon = parameters.epsilon.value\n",
    "    values = np.array(parameters.values) # copy essentially prevents us from accidentally overwriting the original values\n",
    "    # pull out the trial information\n",
    "    stimulus = np.array(trial.get('trials'))\n",
    "    feedback = np.array(trial.get(\"feedback\"))\n",
    "    teacher = np.zeros(4) # teaching signal, reward signal for the learning term\n",
    "    mute = np.zeros(4)  # mute learning for all cues not presented\n",
    "\n",
    "    # activate the value of each available action\n",
    "    # here there are two possible actions, that can take up on 4 different values\n",
    "    # so we subset the values to only include the ones that are activated...\n",
    "    # ...according to which stimuli was presented\n",
    "    activation = values[stimulus - 1]\n",
    "    # convert the activations to a 2x1 matrix, where rows are actions/outcomes\n",
    "    activations = activation.reshape(2, 1)\n",
    "    # calculate a policy based on the activations\n",
    "    response = decision.GreedyRule(activations=activations, epsilon=epsilon)\n",
    "    response.compute() # compute the policy\n",
    "    choice = response.choice() # make a choice\n",
    "    reward = feedback[choice] # get the reward of the chosen action\n",
    "\n",
    "    \n",
    "    # update the value of the chosen action\n",
    "    teacher[stimulus[choice] - 1] = reward # update the teacher's value of the chosen action\n",
    "    mute[stimulus[choice] - 1] = 1 # unmute the learning for the chosen action\n",
    "    update = learning.SeparableRule(weights=values, feedback=teacher, input=mute, alpha=alpha)\n",
    "    update.compute()\n",
    "    values += update.weights.flatten()\n",
    "    ## compile output\n",
    "    output = {\n",
    "        \"policy\"   : response.policies,         # policies\n",
    "        \"response\" : choice,                    # choice based on the policy\n",
    "        \"reward\"   : reward,                    # reward of the chosen action\n",
    "        \"values\"   : values,                    # updated values\n",
    "        \"change\"   : update.weights,            # change in the values\n",
    "        \"activation\" : activations.flatten(),     # activation of the values\n",
    "        \"dependent\"  : np.asarray([choice]),        # dependent variable\n",
    "    }\n",
    "    return output\n",
    "\n",
    "model(parameters, trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important bit here is that `values`, `response` and `policy` must be returned by the function you specify.\n",
    "They will be used by other methods in the toolbox to indentify key variables.\n",
    "\n",
    "`values` and `policy` must be numpy arrays, and `response` could be either an array, int, or float, depending on what the dependent variable is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpm.generators import Simulator, Wrapper\n",
    "\n",
    "wrapper = Wrapper(model=model, parameters=parameters, data=experiment[0])\n",
    "wrapper.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ran the model on a single session, let's investagete the output.\n",
    "`cpm` is great for this because everything that you might need to look at is stored inside the `Wrapper` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "      \"Action value associated with stimuli at the end of training\":\n",
      "          array([0.98158979, 0.06962524, 0.06127021, 0.06962524]),\n",
      "  }\n",
      "{\n",
      "      \"Model output on last trial\":\n",
      "          {\n",
      "              \"policy\": array([0.1, 0.9]),\n",
      "              \"response\"  : 1,\n",
      "              \"reward\": 1.0,\n",
      "              \"values\": array([0.98158979, 0.06962524, 0.06127021, 0.06962524]),\n",
      "              \"change\": array([[0.00251048, 0.        , 0.        , 0.        ]]),\n",
      "              \"activation\": array([0.06962524, 0.97907931]),\n",
      "              \"dependent\" : array([1]),\n",
      "          },\n",
      "  }\n"
     ]
    }
   ],
   "source": [
    "pprint({\"Action value associated with stimuli at the end of training\" : wrapper.values}, depth = 2)\n",
    "pprint({\"Model output on last trial\" : wrapper.simulation[len(wrapper.simulation) - 1]}, depth = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do a complete experiment with 100 participants, and then we will look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/cpm/generators/simulator.py:58: UserWarning: The number of parameter sets and number of participants in data do not match.\n",
      "Using the same parameters for all participants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "simulation = Simulator(model=wrapper, parameters=parameters, data=experiment)\n",
    "\n",
    "simulation.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = simulation.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>policy_0</th>\n",
       "      <th>policy_1</th>\n",
       "      <th>response_0</th>\n",
       "      <th>reward_0</th>\n",
       "      <th>values_0</th>\n",
       "      <th>values_1</th>\n",
       "      <th>values_2</th>\n",
       "      <th>values_3</th>\n",
       "      <th>change_0</th>\n",
       "      <th>change_1</th>\n",
       "      <th>change_2</th>\n",
       "      <th>change_3</th>\n",
       "      <th>activation_0</th>\n",
       "      <th>activation_1</th>\n",
       "      <th>dependent_0</th>\n",
       "      <th>ppt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.0300</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.1936</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.0264</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.968302</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.963980</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.972106</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.003804</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.968302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.972106</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975453</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.972106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.978399</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.975453</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  policy_0  policy_1  response_0  reward_0  values_0  values_1  \\\n",
       "0          0       0.5       0.5           1       1.0  0.250000    0.2200   \n",
       "1          0       0.1       0.9           1       0.0  0.250000    0.2200   \n",
       "2          0       0.1       0.9           1       1.0  0.250000    0.2200   \n",
       "3          0       0.1       0.9           1       1.0  0.340000    0.2200   \n",
       "4          0       0.5       0.5           1       1.0  0.340000    0.1936   \n",
       "...      ...       ...       ...         ...       ...       ...       ...   \n",
       "11995      0       0.0       1.0           1       1.0  0.968302    0.0000   \n",
       "11996      0       1.0       0.0           0       1.0  0.972106    0.0000   \n",
       "11997      0       0.5       0.5           1       1.0  0.972106    0.0000   \n",
       "11998      0       1.0       0.0           0       1.0  0.975453    0.0000   \n",
       "11999      0       0.0       1.0           1       1.0  0.978399    0.0000   \n",
       "\n",
       "       values_2  values_3  change_0  change_1  change_2  change_3  \\\n",
       "0          0.25      0.25 -0.000000   -0.0300     -0.00     -0.00   \n",
       "1          0.25      0.22 -0.000000   -0.0000     -0.00     -0.03   \n",
       "2          0.22      0.22 -0.000000   -0.0000     -0.03     -0.00   \n",
       "3          0.22      0.22  0.090000    0.0000      0.00      0.00   \n",
       "4          0.22      0.22 -0.000000   -0.0264     -0.00     -0.00   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "11995      0.00      0.00  0.004322    0.0000      0.00      0.00   \n",
       "11996      0.00      0.00  0.003804    0.0000      0.00      0.00   \n",
       "11997      0.00      0.00 -0.000000    0.0000      0.00      0.00   \n",
       "11998      0.00      0.00  0.003347    0.0000      0.00      0.00   \n",
       "11999      0.00      0.00  0.002946    0.0000      0.00      0.00   \n",
       "\n",
       "       activation_0  activation_1  dependent_0  ppt  \n",
       "0          0.250000      0.250000            1    0  \n",
       "1          0.220000      0.250000            1    0  \n",
       "2          0.220000      0.250000            1    0  \n",
       "3          0.220000      0.250000            1    0  \n",
       "4          0.220000      0.220000            1    0  \n",
       "...             ...           ...          ...  ...  \n",
       "11995      0.000000      0.963980            1   99  \n",
       "11996      0.968302      0.000000            0   99  \n",
       "11997      0.000000      0.000000            1   99  \n",
       "11998      0.972106      0.000000            0   99  \n",
       "11999      0.000000      0.975453            1   99  \n",
       "\n",
       "[12000 rows x 17 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpm.evaluation import ParameterRecovery, strategies\n",
    "from cpm.optimisation import minimise, DifferentialEvolution\n",
    "\n",
    "recovery = ParameterRecovery(\n",
    "    data=experiment, \n",
    "    model=simulation, \n",
    "    optimiser=DifferentialEvolution,\n",
    "    loss=minimise,\n",
    "    strategies=strategies.grid,\n",
    "    parameters=parameters,\n",
    "    iteration=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recovery.recover()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
