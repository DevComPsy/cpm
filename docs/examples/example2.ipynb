{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Reinforcement learning with a two-armed bandit.\n",
    "\n",
    "This is an **intermediate** level tutorial, where we assume that you are familiar with the basics of reinforcement learning and the two-armed bandit problem.\n",
    "In this example, we will apply and fit a reinforcement learning model to a two-armed bandit problem.\n",
    "The model will be consist of a $\\epsilon$-greedy policy and a prediction error term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-armed bandits\n",
    "\n",
    "Multi-armed bandit problems are a class of reinforcement learning problems where the person is faced with multiple choices, each with a different degree of reward.\n",
    "The goal of the person is to learn which choice is the best and to maximize the reward over time.\n",
    "In this example, we will consider a two-armed bandit problem, where the person is faced with two choices (select an item on the left or the item on the right), each with a different reward.\n",
    "There are 4 different items that can appear in combinations of two, and the reward for each item varies.\n",
    "For example, if item 1 has a chance of 0.7 of giving a reward, then you can expect to receive a reward 70% of the time when you select item 1.\n",
    "The problem is that the underlying structure of the item-reward mapping is unknown.\n",
    "Here we will use the following item-reward mapping:\n",
    "\n",
    "| Item | Reward |\n",
    "|------|--------|\n",
    "| 1    | 0.8    |\n",
    "| 2    | 0.2    |\n",
    "| 3    | 0.5    |\n",
    "| 4    | 0.9    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "\n",
    "First, we will import the data and get it ready for the toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stim_left</th>\n",
       "      <th>stim_right</th>\n",
       "      <th>reward_left</th>\n",
       "      <th>reward_right</th>\n",
       "      <th>ppt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stim_left  stim_right  reward_left  reward_right  ppt\n",
       "0          4           2          0.0           1.0    1\n",
       "1          2           4          1.0           0.0    1\n",
       "2          2           3          1.0           1.0    1\n",
       "3          4           1          0.0           1.0    1\n",
       "4          4           2          0.0           1.0    1\n",
       "5          4           3          0.0           1.0    1\n",
       "6          2           1          1.0           1.0    1\n",
       "7          3           2          1.0           1.0    1\n",
       "8          3           2          1.0           1.0    1\n",
       "9          3           1          1.0           1.0    1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "experiment = pd.read_csv('bandit.csv', header=0)\n",
    "experiment.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at what each column represents:\n",
    "\n",
    "- `left`: the stimulus presented on the left side.\n",
    "- `right`: the stimulus presented on the right side.\n",
    "- `reward_left`: the reward received when the left stimulus is selected.\n",
    "- `reward_right`: the reward received when the right stimulus is selected.\n",
    "- `ppt`: the participant number.\n",
    "\n",
    "Notice that for now, we don't have any actual data recorded from participants.\n",
    "That is because, for now, we will only import the environment, containing all states and rewards.\n",
    "So, let us convert the data into a format that the toolbox can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, the toolbox provides a function to convert the data into the required format.\n",
    "We will use the `pandas_to_dict` function available in the `cpm.utils` module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants: 100\n"
     ]
    }
   ],
   "source": [
    "from cpm.utils import pandas_to_dict\n",
    "\n",
    "experiment = pandas_to_dict(experiment, participant=\"ppt\", stimuli='stim', feedback='reward')\n",
    "length = len(experiment)\n",
    "print(f\"Number of participants: {length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have a list of dictionaries, where each dictionary represents an experimental session that a participant might complete.\n",
    "If you have 100 participants or sessions, then you will have 100 dictionaries in the list -each with their unique trial order (schedule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key variables in the dictionary: dict_keys(['trials', 'feedback'])\n",
      "Number of trials and number of the stimuli on each of those trials: (60, 2)\n",
      "Number of trials and number of the feedback on each of those trials: (60, 2)\n",
      "All looks good! We are ready to go!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Key variables in the dictionary: {experiment[0].keys()}\")\n",
    "print(f\"Number of trials and number of the stimuli on each of those trials: {experiment[0].get('trials').shape}\")\n",
    "print(f\"Number of trials and number of the feedback on each of those trials: {experiment[0].get('feedback').shape}\")\n",
    "print(\"All looks good! We are ready to go!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us see what each value within the dictionary represents:\n",
    "\n",
    "- `input`: the stimuli presented in the session. It must bve a numpy.ndarray. Each row represents a trial, and the columns represent the left and right stimuli presented in each trial.\n",
    "- `feedback`: the rewards that could be obtained from the stimuli. It must be a numpy.ndarray. Each row represents a trial, and the columns represent the reward that could be obtained from the corresponding stimulus in the `stimuli` key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "Let us quickly go through the model we will use.\n",
    "\n",
    "Each stimulus has an associated value, which is the expected reward that can be obtained from selecting that stimulus.\n",
    "\n",
    "Let $Q(a)$ be the estimated value of action $a$.\n",
    "On each trial, $t$, there are two stimuli present, so that $Q(a)$ could be $Q(\\text{left})$ or $Q(\\text{right})$, where the corresponding Q-values are derived from the associated value of the stimuli present on left or right.\n",
    "\n",
    "On each trial $t$, the $\\epsilon$-greedy policy selects a random action with probability $\\epsilon$, the exploration rate parameter, and selects the action with the highest estimated value with probability $1-n\\epsilon$, where $n$ the number of possible actions.\n",
    "So, on each trial, the model will select an action (left or right) based on the following policy:\n",
    "\n",
    "$$\n",
    "A_t = \n",
    "\\begin{cases} \n",
    "\\text{random action} & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max_a Q_t(a) & \\text{with probability } 1 - n \\epsilon \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $A_t$ is the action selected at time $t$, and $Q_t(a)$ is the estimated value of action $a$ at time $t$.\n",
    "\n",
    "The model will update the estimated value of the selected action using the following learning rule:\n",
    "\n",
    "$$\n",
    "\\Delta Q_t(A_t) = \\alpha \\times \\Big[ R_t - Q_t(A_t) \\Big]\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, and $R_t$ is the reward received at time $t$.\n",
    "Q-values are then updated as follows:\n",
    "\n",
    "$$\n",
    "Q_{t+1}(A_t) = Q_t(A_t) + \\Delta Q_t(A_t)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Building the model\n",
    "\n",
    "In order to use the toolbox, you will have to specify **the computations for one single trial**.\n",
    "Rest assured, you do not have to build the model from scratch.\n",
    "We have fully-fledged models in `cpm.applications` that you can use, but we also have all the building blocks in `cpm.components` that you can use to build your own model.\n",
    "\n",
    "For now, let us simplify the problem and start by specifying what information we need on each trial.\n",
    "This information will usually be extracted from the data we just imported.\n",
    "Here, we create this to help us develop the model.\n",
    "\n",
    "Here, we need to specify the following:\n",
    "\n",
    "- `stimuli`: the stimuli presented in the trial.\n",
    "- `rewards`: the rewards that could be obtained from selecting the stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = {\n",
    "    \"trials\": [1, 4],\n",
    "    \"feedback\": [1, 0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we build the model, we also have to talk about the `cpm.Parameter` class.\n",
    "This class is used to specify the parameters of the model, including various bounds and priors.\n",
    "Let us specify the parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The learning rate of the model: {'value': 0.12, 'prior': 'normal', 'lower': 0, 'upper': 1}\n",
      "The exploration rate of the model: {'value': 0.1, 'prior': 'normal', 'lower': 0, 'upper': 1}\n",
      "The initial value of each action: {'value': array([0.5, 0. , 0. , 0.2]), 'prior': 'normal', 'lower': 0, 'upper': 1}\n"
     ]
    }
   ],
   "source": [
    "from cpm.models import Parameters\n",
    "\n",
    "parameters = Parameters(alpha = 0.12, epsilon = 0.1, values = np.array([0.5, 0.0, 0.0, 0.2]))\n",
    "print(f\"The learning rate of the model: {parameters.alpha.export()}\")\n",
    "print(f\"The exploration rate of the model: {parameters.epsilon.export()}\")\n",
    "print(f\"The initial value of each action: {parameters.values.export()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can immediately see that the toolbox defined a bunch of things for us.\n",
    "This includes the prior and the parameter ranges as well.\n",
    "One thing we have to clarify is the value for each action.\n",
    "Here we initialized the value of each action to 0.\n",
    "This is somewhat special here, because it is a 2D array.\n",
    "The reason is because each stimuli is actionable, and each stimuli is a single unit.\n",
    "If we had compound stimulus on each side, varying on various dimensions (color, shape, etc.), then we would have multiple columns instead of just one.\n",
    "One such example is Niv et al. (2015), where they had three compound stimuli on the screen, that varied on dimensions of color, shape, and fill type.\n",
    "The way you represent it will also depend on the model building blocks you use and how you specify the computations for each trial.\n",
    "Nonetheless, even though we treated it as a parameter, we do not need to estimate it later on.\n",
    "\n",
    "That was enough preparation, let us build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 45\u001b[0m\n\u001b[1;32m     35\u001b[0m     output \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m   : response\u001b[38;5;241m.\u001b[39mpolicies,         \u001b[38;5;66;03m# policies\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m : choice,                    \u001b[38;5;66;03m# choice based on the policy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m\"\u001b[39m: activations\u001b[38;5;241m.\u001b[39mflatten()     \u001b[38;5;66;03m# activation of the values\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     }\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m---> 45\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(parameters, trial)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# pull out the trial information\u001b[39;00m\n\u001b[1;32m      9\u001b[0m stimulus \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(trial\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m---> 10\u001b[0m feedback \u001b[38;5;241m=\u001b[39m \u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeedback\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m()\n\u001b[1;32m     11\u001b[0m teacher \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# teaching signal for the learning term\u001b[39;00m\n\u001b[1;32m     12\u001b[0m mute \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# mute learning for all cues not presented\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "from cpm.components import learning, decision, utils\n",
    "\n",
    "def model(parameters, trial):\n",
    "    # pull out the parameters\n",
    "    alpha = parameters.alpha\n",
    "    epsilon = parameters.epsilon.value\n",
    "    values = parameters.values.value.copy()\n",
    "    # pull out the trial information\n",
    "    stimulus = np.array(trial.get('trials'))\n",
    "    feedback = np.array(trial.get(\"feedback\"))\n",
    "    teacher = np.zeros(4) # teaching signal for the learning term\n",
    "    mute = np.zeros(4)  # mute learning for all cues not presented\n",
    "\n",
    "    # activate the value of each available action\n",
    "    # here there are two possible actions, that can take up on 4 different values\n",
    "    # so we subset the values to only include the ones that are activated...\n",
    "    # ...according to which stimuli was presented\n",
    "    activation = values[stimulus - 1]\n",
    "    # convert the activations to a 2x1 matrix, where rows are actions/outcomes\n",
    "    activations = activation.reshape(2, 1)\n",
    "    # calculate a policy based on the activations\n",
    "    response = decision.GreedyRule(activations=activations, epsilon=epsilon)\n",
    "    response.compute() # compute the policy\n",
    "    choice = response.choice() # make a choice\n",
    "    reward = feedback[choice] # get the reward of the chosen action\n",
    "\n",
    "    \n",
    "    # update the value of the chosen action\n",
    "    teacher[stimulus[choice] - 1] = reward # update the teacher's value of the chosen action\n",
    "    mute[stimulus[choice] - 1] = 1 # unmute the learning for the chosen action\n",
    "    update = learning.SeparableRule(weights=values, feedback=teacher, input=mute, alpha=alpha)\n",
    "    update.compute()\n",
    "    values += update.weights.flatten()\n",
    "    ## compile output\n",
    "    output = {\n",
    "        \"policy\"   : response.policies,         # policies\n",
    "        \"response\" : choice,                    # choice based on the policy\n",
    "        \"reward\"   : reward,                    # reward of the chosen action\n",
    "        \"values\"   : values,                    # updated values\n",
    "        \"change\"   : update.weights,            # change in the values\n",
    "        \"activation\": activations.flatten()     # activation of the values\n",
    "    }\n",
    "    return output\n",
    "\n",
    "model(parameters, trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important bit here is that values, response and policy must be returned by the function you specify.\n",
    "They will be used by other methods in the toolbox to indentify key variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcpm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Simulator, Wrapper\n\u001b[1;32m      3\u001b[0m wrapper \u001b[38;5;241m=\u001b[39m Wrapper(model\u001b[38;5;241m=\u001b[39mmodel, parameters\u001b[38;5;241m=\u001b[39mparameters, data\u001b[38;5;241m=\u001b[39mexperiment[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m \u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/cpm/models/wrapper.py:82\u001b[0m, in \u001b[0;36mWrapper.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)):\n\u001b[1;32m     78\u001b[0m     trial \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining[i]),\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeedback\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39masarray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeedback[i]]),\n\u001b[1;32m     81\u001b[0m     }\n\u001b[0;32m---> 82\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# output.compute(**trial)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 29\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(parameters, trial)\u001b[0m\n\u001b[1;32m     25\u001b[0m reward \u001b[38;5;241m=\u001b[39m feedback[choice] \u001b[38;5;66;03m# get the reward of the chosen action\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# update the value of the chosen action\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mteacher\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstimulus\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;66;03m# update the teacher's value of the chosen action\u001b[39;00m\n\u001b[1;32m     30\u001b[0m mute[stimulus[choice] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# unmute the learning for the chosen action\u001b[39;00m\n\u001b[1;32m     31\u001b[0m update \u001b[38;5;241m=\u001b[39m learning\u001b[38;5;241m.\u001b[39mSeparableRule(weights\u001b[38;5;241m=\u001b[39mvalues, feedback\u001b[38;5;241m=\u001b[39mteacher, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmute, alpha\u001b[38;5;241m=\u001b[39malpha)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from cpm.models import Simulator, Wrapper\n",
    "\n",
    "wrapper = Wrapper(model=model, parameters=parameters, data=experiment[0])\n",
    "wrapper.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Recovery"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
