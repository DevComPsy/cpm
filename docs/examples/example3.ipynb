{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Estimating Empirical Priors\n",
    "\n",
    "In the following example, we will walkthrough how to estimate empirical priors for reinforcement learning model based on Gershman (2016).\n",
    "We will apply this for the 2-armed bandit task we looked at in the previous example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at what each column represents:\n",
    "\n",
    "- `index`: variable to identify each row - this variable is clutter.\n",
    "- `left`: the stimulus presented on the left side.\n",
    "- `right`: the stimulus presented on the right side.\n",
    "- `reward_left`: the reward received when the left stimulus is selected.\n",
    "- `reward_right`: the reward received when the right stimulus is selected.\n",
    "- `ppt`: the participant number.\n",
    "- `responses`: the response of the participant (1 for right, 0 for left)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "\n",
    "First, we will import the data and get it ready for the toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cmx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettyformatter import pprint\n",
    "\n",
    "experiment = pd.read_csv('bandit_small.csv', header=0)\n",
    "experiment.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will quickly add a column `observed` to specify for the toolbox, what is the dependent variable we actually want to predict.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment['observed'] = experiment['responses'] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "Let us quickly recap through the model we will use.\n",
    "\n",
    "Each stimulus has an associated value, which is the expected reward that can be obtained from selecting that stimulus.\n",
    "\n",
    "Let $Q(a)$ be the estimated value of action $a$.\n",
    "On each trial, $t$, there are two stimuli present, so that $Q(a)$ could be $Q(\\text{left})$ or $Q(\\text{right})$, where the corresponding Q-values are derived from the associated value of the stimuli present on left or right.\n",
    "More formally, we can say that the expected value of the action $a$ selected at time $t$ is given by:\n",
    "\n",
    "$$\n",
    "Q_t(a) = \\mathbb{E}[R_t | A_t = a]\n",
    "$$\n",
    "\n",
    "where $R_t$ is the reward received at time $t$, and $A_t$ is the action selected at time $t$.\n",
    "\n",
    "On each trial $t$, the Softmax policy will select an action (left or right) based on the following policy:\n",
    "\n",
    "$$\n",
    "P(a_t) = \\frac{e^{Q_{a,t} \\epsilon}}{\\sum_{i = 1}^{k}{e^{Q_{i,t} \\epsilon}}}\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is the inverse temperature parameter, and $Q_{a,t}$ is the estimated value of action $a$ at time $t$.\n",
    "$k$ is the number of actions available, and in our case, $k = 2$.\n",
    "So, on each trial, the model will select an action (left or right) based on the following policy:\n",
    "\n",
    "$$\n",
    "A_t = \\begin{cases}\n",
    "\\text{left} & \\text{with probability } P(a_{\\text{left}}) \\\\\n",
    "\\text{right} & \\text{with probability } P(a_{\\text{right}})\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $A_t$ is the action selected at time $t$, and $Q_t(a)$ is the estimated value of action $a$ at time $t$.\n",
    "\n",
    "The model will calculate the prediction error according to the following learning rule:\n",
    "\n",
    "$$\n",
    "\\Delta Q_t(A_t) = \\alpha \\times \\Big[ R_t - Q_t(A_t) \\Big]\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, and $R_t$ is the reward received at time $t$.\n",
    "Note that this rule is often reported as the Rescorla-Wagner learning rule, where for each action/outcome, the prediction error is the difference between the reward received and the sum of all values present on each trial for that action.\n",
    "In our case, we only have one value for each action, so the Rescorla-Wagner learning rule is reduced to the Bush and Mosteller separable error term.\n",
    "Q-values are then updated as follows:\n",
    "\n",
    "$$\n",
    "Q_{t+1}(A_t) = Q_t(A_t) + \\Delta Q_t(A_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpm.generators import Parameters, Value\n",
    "\n",
    "## define our parameters\n",
    "parameters = Parameters(\n",
    "    # freely varying parameters are indicated by specifying priors\n",
    "    alpha=Value(\n",
    "        value=0.5,\n",
    "        lower=1e-10,\n",
    "        upper=1,\n",
    "        prior=\"truncated_normal\",\n",
    "        args={\"mean\": 0.6, \"sd\": 0.20}, # we pick some random values here for the true prior\n",
    "    ),\n",
    "    temperature=Value(\n",
    "        value=1,\n",
    "        lower=0,\n",
    "        upper=10,\n",
    "        prior=\"truncated_normal\",\n",
    "        args={\"mean\": 2, \"sd\": 2.5}, # we pick some random values here for the true prior\n",
    "    ),\n",
    "    # everything without a prior is part of the initial state of the model or fixed constructs (e.g. exemplars in general-context models of categorization)\n",
    "    values = np.array([0.25, 0.25, 0.25, 0.25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpm.models import learning, decision, utils\n",
    "\n",
    "## define our models\n",
    "def model(parameters, trial):\n",
    "    # pull out the parameters\n",
    "    alpha = parameters.alpha\n",
    "    temperature = parameters.temperature\n",
    "    values = np.array(parameters.values) # copy essentially prevents us from accidentally overwriting the original values\n",
    "    # pull out the trial information\n",
    "    stimulus = np.array([trial.stimulus_left, trial.stimulus_right]).astype(int)\n",
    "    feedback = np.array([trial.reward_left, trial.reward_right])\n",
    "    human_choice = trial.responses.astype(int) - 1 # convert to 0-indexing\n",
    "    mute = np.zeros(4)  # mute learning for all cues not presented\n",
    "\n",
    "    # activate the value of each available action\n",
    "    # here there are two possible actions, that can take up on 4 different values\n",
    "    # so we subset the values to only include the ones that are activated...\n",
    "    # ...according to which stimuli was presented\n",
    "    activation = values[stimulus - 1]\n",
    "    # convert the activations to a 2x1 matrix, where rows are actions/outcomes\n",
    "    activations = activation.reshape(2, 1)\n",
    "    # calculate a policy based on the activations\n",
    "    response = decision.Softmax(activations=activations, temperature=temperature)\n",
    "    response.compute() # compute the policy\n",
    "    if np.isnan(response.policies).any():\n",
    "        # if the policy is NaN for a given action, then we need to set it to 1\n",
    "        print(f\"NaN in policy with parameters: {alpha.value}, {epsilon.value}\\n\")\n",
    "        print(response.policies)\n",
    "        response.policies[np.isnan(response.policies)] = 1\n",
    "\n",
    "    model_choice = response.choice() # get the choice based on the policy\n",
    "    reward = feedback[human_choice] # get the reward of the chosen action\n",
    "    # update the value of the chosen action\n",
    "    mute[stimulus[human_choice] - 1] = 1 # unmute the learning for the chosen action\n",
    "    teacher = np.array([reward])\n",
    "    update = learning.SeparableRule(weights=values, feedback=teacher, input=mute, alpha=alpha)\n",
    "    update.compute()\n",
    "    values += update.weights.flatten()\n",
    "    ## compile output\n",
    "    output = {\n",
    "        \"trial\"    : trial.trial.astype(int),               # trial number\n",
    "        \"policy\"   : response.policies,         # policies\n",
    "        \"response\" : model_choice,              # choice based on the policy\n",
    "        \"reward\"   : reward,                    # reward of the chosen action\n",
    "        \"values\"   : values,                    # updated values\n",
    "        \"change\"   : update.weights,            # change in the values\n",
    "        \"activation\" : activations.flatten(),     # activation of the values\n",
    "        \"dependent\"  : np.array([response.policies[1]]),        # dependent variable\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up wrappers and run the model to check for bugs\n",
    "from cpm.generators import Simulator, Wrapper\n",
    "\n",
    "wrapper = Wrapper(model=model, parameters=parameters, data=experiment[experiment.ppt == 1])\n",
    "wrapper.run() # run the model\n",
    "\n",
    "output = wrapper.export() # export sim data into pandas dataframe\n",
    "output.tail() # check for output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate the data\n",
    "\n",
    "In this section, we will generate some data from the model we defined above.\n",
    "We will use the toolbox to simulate the data and then estimate the empirical priors from the simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = parameters.sample(experiment.shape[0]) # sample a set of parameters\n",
    "\n",
    "simulator = Simulator(wrapper=wrapper,\n",
    "                      parameters=fit.parameters, # parameters from the optimiser object is directly accessible for simulation\n",
    "                      data=experiment.groupby(\"ppt\")) # data grouped by participants\n",
    "simulator.run()\n",
    "\n",
    "generated = simulator.export()\n",
    "experiment.response = generated.responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the hierarchical vs non-hierarchical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up the fits\n",
    "from cpm.optimisation import minimise, FminBound\n",
    "\n",
    "fit = FminBound(\n",
    "    model=wrapper,  # Wrapper class with the model we specified from before\n",
    "    data=experiment,  # the data as a list of dictionaries\n",
    "    minimisation=minimise.LogLikelihood.bernoulli,\n",
    "    parallel=False,\n",
    "    prior=False,\n",
    "    ppt_identifier=\"ppt\",\n",
    "    display=False,\n",
    "    number_of_starts=5,\n",
    "    # everything below is optional and passed directly to the scipy implementation of the optimiser\n",
    "    approx_grad=True\n",
    ")\n",
    "fit.optimise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we now set up the hierarchical fitting\n",
    "from cpm.hierarchical import EmpiricalBayes as hierarchical\n",
    "\n",
    "fit_with_prior = FminBound(\n",
    "    model=wrapper,  # Wrapper class with the model we specified from before\n",
    "    data=experiment,  # the data as a list of dictionaries\n",
    "    minimisation=minimise.LogLikelihood.bernoulli,\n",
    "    parallel=False,\n",
    "    prior=True,\n",
    "    ppt_identifier=\"ppt\",\n",
    "    display=False,\n",
    "    number_of_starts=5,\n",
    "    # everything below is optional and passed directly to the scipy implementation of the optimiser\n",
    "    approx_grad=True\n",
    ")\n",
    "eb = hierarchical(optimiser=fit, iteration=20, chain=2)\n",
    "# disable numpy errors - there is some out-of-bounds errors in the softmax function that we can ignore\n",
    "np.seterr(all=\"ignore\")\n",
    "eb.optimise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will extract the best performing hyperparameters for the priors and reinitialise the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = eb.hyperparameters\n",
    "hyper = hyperparameters[hyperparameters.lme == hyperparameters.lme.max()]\n",
    "means = [hyper[hyper.parameter == \"alpha\"].squeeze()['mean'],\n",
    "         hyper[hyper.parameter == \"beta\"].squeeze()['mean']]\n",
    "\n",
    "ssd = [hyper[hyper.parameter == \"alpha\"].squeeze()['sd'],\n",
    "       hyper[hyper.parameter == \"beta\"].squeeze()['sd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## redefine parameters\n",
    "estimated_parameters = Parameters(\n",
    "    # freely varying parameters are indicated by specifying priors\n",
    "    alpha=Value(\n",
    "        value=0.5,\n",
    "        lower=1e-10,\n",
    "        upper=1,\n",
    "        prior=\"truncated_normal\",\n",
    "        args={\"mean\": means[0], \"sd\": ssd[0]},\n",
    "    ),\n",
    "    temperature=Value(\n",
    "        value=1,\n",
    "        lower=0,\n",
    "        upper=10,\n",
    "        prior=\"truncated_normal\",\n",
    "        args={\"mean\": means[1], \"sd\": ssd[1]},\n",
    "    ),\n",
    "    # everything without a prior is part of the initial state of the model or fixed constructs (e.g. exemplars in general-context models of categorization)\n",
    "    values = np.array([0.25, 0.25, 0.25, 0.25]))\n",
    "\n",
    "## reinitialise the model\n",
    "wrapper_with_prior = Wrapper(model=model, parameters=estimated_parameters, data=experiment[experiment.ppt == 1])\n",
    "fit_with_prior = FminBound(\n",
    "    model=wrapper,  # Wrapper class with the model we specified from before\n",
    "    data=experiment,  # the data as a list of dictionaries\n",
    "    minimisation=minimise.LogLikelihood.bernoulli,\n",
    "    parallel=False,\n",
    "    prior=True,\n",
    "    ppt_identifier=\"ppt\",\n",
    "    display=False,\n",
    "    number_of_starts=5,\n",
    "    # everything below is optional and passed directly to the scipy implementation of the optimiser\n",
    "    approx_grad=True\n",
    ")\n",
    "fit_with_prior.optimise()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
