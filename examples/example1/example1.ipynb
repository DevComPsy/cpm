{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: An associative learning model and blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a beginner's walkthrough of an associative learning model and the blocking effect. The model is an instance of the Rescorla-Wagner learning rule (Rescorla and Wagner, 1972), which is a simple and widely used model of associative learning. The blocking effect (Kamin, 1961) is a well-known phenomenon in the psychology of learning, and it was the original target phenomena for the Rescorla-Wagner model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Blocking Effect\n",
    "\n",
    "The blocking effect (Kamin, 1961) is a type of cue competition when learning about one cue is restricted in the presence of another cue that was trained separately.\n",
    "In a simple example, consider a rat that has been trained to associate light (A) with food (+). and subsequently encounters the compound of light and sound (AB) with food (+). Under these conditions, learning about B is restricted.\n",
    "In humans, learning (in associative or contingency learning experiments) is often measured by asking them to rate the likelihood of an outcome (e.g., food) given a cue (e.g., light).\n",
    "Participants rate blocked cues (B) as less likely to result in an outcome than a control cue (e.g. X following Y- and XY+ training).\n",
    "This is the blocking effect.\n",
    "\n",
    "In this tutorial, we will use the toolbox to fit the Bush and Mosteller (1951) separable error term and the Rescorla-Wagner (1972) summed-error term to data from a blocking experiment.\n",
    "It is a partial recreation of Spicer et al. (2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data and getting it ready for the toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from prettyformatter import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our two models\n",
    "\n",
    "Here we will look at one bad model and one good model of blocking.\n",
    "\n",
    "In both models, the error is scaled by a global and stimulus-specific learning rates (sometimes thought to represent cue salience).\n",
    "For simplicity, we will assume that the learning rate is the same for all cues and all participants, so we will use a single global learning rate, $\\alpha$ and assume that all stimuli have the same salience, therefore we can omit that parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bush and Mosteller (1951) rule\n",
    "\n",
    "\n",
    "$$\n",
    "\\Delta V_i = \\alpha (R - V_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rescorla-Wagner (1972) rule\n",
    "\n",
    "$$\n",
    "\\Delta V_i = \\alpha (R - \\sum_{i=0} ^{k} V)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25 0.25 0.25 0.25]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cpm.models import learning, utils\n",
    "from cpm.generators import Parameters, Wrapper, Simulator\n",
    "\n",
    "parameters = Parameters(alpha = 0.2, values = np.array([0.25, 0.25, 0.25, 0.25]))\n",
    "parameters.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"trials\": np.array([3, 4]),\n",
    "    \"feedback\": np.array([1]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `cpm` toolbox, you will need to write a function that takes in the parameters and the dataand outputs all the things you want to compute on each trial.\n",
    "The parameters must be a `Parameter` object and the input must be a dictionary with the mandatory keys `trials` and `feedback`.\n",
    "The output must be a dictionary with the mandatory keys `values` and `dependent`.\n",
    "Both the `input` and `output` dictionaries can have any other keys you want or need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bush and Mosteller (1951)'s single linear operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"activations\"   : array([0.  , 0.  , 0.25, 0.25]),\n",
      "    \"values\": array([0.25, 0.25, 0.4 , 0.4 ]),\n",
      "    \"policy\": 0.5,\n",
      "    \"error\" : array([0.  , 0.  , 0.15, 0.15]),\n",
      "    \"dependent\"     : 0.5,\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def bush_and_mosteller(parameters, input):\n",
    "    # get parameters\n",
    "    alpha = parameters.alpha\n",
    "    values = np.array(parameters.values)\n",
    "\n",
    "    # get trial-specific inputs\n",
    "    stimulus = input.get(\"trials\").copy()\n",
    "    stimulus = utils.Nominal(target = stimulus, bits = 4)\n",
    "    feedback = input.get(\"feedback\").copy()\n",
    "\n",
    "    # mute all non-presented stimuli values\n",
    "    activations = stimulus * values\n",
    "    # compute what we believe the policy will be\n",
    "    policy = np.sum(activations)\n",
    "    error = learning.SeparableRule(weights=values, feedback=feedback, alpha=alpha, input = stimulus)\n",
    "    error.compute()\n",
    "    values += error.weights[0]\n",
    "    output = {\n",
    "        \"activations\": activations,\n",
    "        \"values\": values,\n",
    "        \"policy\": policy,\n",
    "        \"error\": error.weights[0],\n",
    "        \"dependent\": policy,\n",
    "    }\n",
    "    return output\n",
    "\n",
    "pp(bush_and_mosteller(parameters, input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescorla-Wagner (1972)'s summed error term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"activations\"   : array([0.  , 0.  , 0.25, 0.25]),\n",
      "    \"values\": array([0.25   , 0.25   , 0.29375, 0.29375]),\n",
      "    \"policy\": 0.5,\n",
      "    \"error\" : array([0.     , 0.     , 0.04375, 0.04375]),\n",
      "    \"dependent\"     : 0.5,\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def rescorla_wagner(parameters, input):\n",
    "    # get parameters\n",
    "    alpha = parameters.alpha\n",
    "    values = np.array(parameters.values) # copy to avoid changing the original\n",
    "\n",
    "    # get trial-specific inputs\n",
    "    stimulus = input.get(\"trials\").copy()\n",
    "    stimulus = utils.Nominal(target = stimulus, bits = 4)\n",
    "    feedback = input.get(\"feedback\").copy()\n",
    "\n",
    "    # mute all non-presented stimuli values\n",
    "    activations = stimulus * values\n",
    "    # compute what we believe the policy will be\n",
    "    policy = np.sum(activations)\n",
    "    error = learning.DeltaRule(weights=values, feedback=feedback, alpha=alpha, input = activations)\n",
    "    error.compute()\n",
    "    values += error.weights[0]\n",
    "    output = {\n",
    "        \"activations\": activations,\n",
    "        \"values\": values,\n",
    "        \"policy\": policy,\n",
    "        \"error\": error.weights[0],\n",
    "        \"dependent\": policy,\n",
    "    }\n",
    "    return output\n",
    "\n",
    "pp(rescorla_wagner(parameters, input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
