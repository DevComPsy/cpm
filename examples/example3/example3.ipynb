{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Estimating Empirical Priors\n",
    "\n",
    "In the following example, we will walkthrough how to estimate empirical priors for reinforcement learning model based on Gershman (2016).\n",
    "We will apply this for the 2-armed bandit task we looked at in the previous example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at what each column represents:\n",
    "\n",
    "- `index`: variable to identify each row - this variable is clutter.\n",
    "- `left`: the stimulus presented on the left side.\n",
    "- `right`: the stimulus presented on the right side.\n",
    "- `reward_left`: the reward received when the left stimulus is selected.\n",
    "- `reward_right`: the reward received when the right stimulus is selected.\n",
    "- `ppt`: the participant number.\n",
    "- `responses`: the response of the participant (1 for right, 0 for left)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "\n",
    "First, we will import the data and get it ready for the toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cmx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettyformatter import pprint\n",
    "\n",
    "experiment = pd.read_csv('bandit_small.csv', header=0)\n",
    "experiment.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will quickly add a column `observed` to specify for the toolbox, what is the dependent variable we actually want to predict.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment['observed'] = experiment['responses'] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "Let us quickly recap through the model we will use.\n",
    "\n",
    "Each stimulus has an associated value, which is the expected reward that can be obtained from selecting that stimulus.\n",
    "\n",
    "Let $Q(a)$ be the estimated value of action $a$.\n",
    "On each trial, $t$, there are two stimuli present, so that $Q(a)$ could be $Q(\\text{left})$ or $Q(\\text{right})$, where the corresponding Q-values are derived from the associated value of the stimuli present on left or right.\n",
    "More formally, we can say that the expected value of the action $a$ selected at time $t$ is given by:\n",
    "\n",
    "$$\n",
    "Q_t(a) = \\mathbb{E}[R_t | A_t = a]\n",
    "$$\n",
    "\n",
    "where $R_t$ is the reward received at time $t$, and $A_t$ is the action selected at time $t$.\n",
    "\n",
    "On each trial $t$, the Softmax policy will select an action (left or right) based on the following policy:\n",
    "\n",
    "$$\n",
    "P(a_t) = \\frac{e^{Q_{a,t} \\epsilon}}{\\sum_{i = 1}^{k}{e^{Q_{i,t} \\epsilon}}}\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is the inverse temperature parameter, and $Q_{a,t}$ is the estimated value of action $a$ at time $t$.\n",
    "$k$ is the number of actions available, and in our case, $k = 2$.\n",
    "So, on each trial, the model will select an action (left or right) based on the following policy:\n",
    "\n",
    "$$\n",
    "A_t = \\begin{cases}\n",
    "\\text{left} & \\text{with probability } P(a_{\\text{left}}) \\\\\n",
    "\\text{right} & \\text{with probability } P(a_{\\text{right}})\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $A_t$ is the action selected at time $t$, and $Q_t(a)$ is the estimated value of action $a$ at time $t$.\n",
    "\n",
    "The model will calculate the prediction error according to the following learning rule:\n",
    "\n",
    "$$\n",
    "\\Delta Q_t(A_t) = \\alpha \\times \\Big[ R_t - Q_t(A_t) \\Big]\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, and $R_t$ is the reward received at time $t$.\n",
    "Note that this rule is often reported as the Rescorla-Wagner learning rule, where for each action/outcome, the prediction error is the difference between the reward received and the sum of all values present on each trial for that action.\n",
    "In our case, we only have one value for each action, so the Rescorla-Wagner learning rule is reduced to the Bush and Mosteller separable error term.\n",
    "Q-values are then updated as follows:\n",
    "\n",
    "$$\n",
    "Q_{t+1}(A_t) = Q_t(A_t) + \\Delta Q_t(A_t)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
