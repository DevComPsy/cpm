{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: scale to hypercomputing cluster\n",
    "\n",
    "CPM does not have a built-in support for hypercomputing clusters, but it is possible to scale the fitting to a cluster by running the script on each node of the cluster.\n",
    "On many occasions, where we have a sufficiently large dataset, we may want to fit the model on a hypercomputing cluster to speed up the process.\n",
    "Here, we will explore how to do this using the `sockets`, a built-in library in Python.\n",
    "\n",
    "This is a crude approach, but it will suit for all day-to-day purposes.\n",
    "What we essentially do, is to divide the data into chunks and fit each chunk on a different node of the cluster.\n",
    "This requires an executable script that can be run on each node of the cluster and can be submitted to a slurm cluster.\n",
    "An example script is provided below, let's call it `my_job.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the current data: (576, 9), which is 25.0% of the complete data allocated to a single node.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3.12\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    import socket\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    data = pd.read_csv('bandit_small.csv')\n",
    "\n",
    "    ## subset data into chunks\n",
    "    ppt_to_chunks = data.ppt.unique()\n",
    "\n",
    "    # Get the number of nodes available\n",
    "    num_nodes = int(os.getenv(\"SLURM_JOB_NUM_NODES\", 1))\n",
    "\n",
    "    # Nodes will always be one as we are running this on a single node\n",
    "    # Remove the following line if you want to run this on multiple nodes\n",
    "    num_nodes = 4\n",
    "\n",
    "    chunks = np.array_split(ppt_to_chunks, num_nodes)\n",
    "\n",
    "    # Get the hostname of the node\n",
    "    node_name = socket.gethostname()\n",
    "\n",
    "    # Get the SLURM task ID\n",
    "    task_id = int(os.getenv(\"SLURM_PROCID\", 0))\n",
    "\n",
    "    # Some useful information to print\n",
    "    # print(f\"Node: {node_name}, Task ID: {task_id}\")\n",
    "\n",
    "    # Get the chunk of data that this node will work on\n",
    "    ppt_to_nodes = data.ppt.isin(chunks[0])\n",
    "\n",
    "    print(f\"Shape of the current data: {data[ppt_to_nodes].shape}, which is {(data[ppt_to_nodes].shape[0] / data.shape[0])*100}% of the complete data allocated to a single node.\")\n",
    "\n",
    "    # Below you can do your job with the data\n",
    "    # do_something(data[ppt_to_nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the script is ready, we will usually have to write a bash script that will submit the job to the cluster.\n",
    "See an example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash -l\n",
    "\n",
    "#### Define some basic SLURM properties for this job - there can be many more!\n",
    "#SBATCH --job-name=my_simulation # Replace with your job name\n",
    "#SBATCH --nodes=4 # Replace with the number of nodes you need\n",
    "#SBATCH --partition compute\n",
    "#SBATCH --output=%x_%j.out\n",
    "#SBATCH --error=%x_%j.err\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --cpus-per-task=64 # Replace with the number of cores you need\n",
    "#SBATCH --time=48:00:00 # Replace with the time you need, the format is hour:minutes:seconds\n",
    "\n",
    "#### This block shows how you would create a working directory to store your job data:\n",
    "# Define and create a unique scratch directory for this job:\n",
    "SCRATCH_DIRECTORY=/${USER}/${SLURM_JOBID}\n",
    "mkdir -p ${SCRATCH_DIRECTORY}/results\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"Failed to create scratch directory\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "cd ${SCRATCH_DIRECTORY}\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"Failed to change directory to scratch directory\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Note: ${SLURM_SUBMIT_DIR} contains the path where you started the job\n",
    "\n",
    "# You can copy everything you need to the scratch directory\n",
    "# ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from\n",
    "cp -r ${SLURM_SUBMIT_DIR}/* ${SCRATCH_DIRECTORY}\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"Failed to copy files to scratch directory\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "#### Do the actual work:\n",
    "# Make sure we have singularity available\n",
    "module load python\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"Failed to load Python module\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Debugging output to verify paths and environment variables\n",
    "echo \"Scratch directory: ${SCRATCH_DIRECTORY}\"\n",
    "echo \"Submit directory: ${SLURM_SUBMIT_DIR}\"\n",
    "\n",
    "# Run the simulation using Singularity\n",
    "srun python my_job.py\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"Singularity execution failed\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Note the hostname command in there - this will print the compute node's name into the output, making it easier to understand what's going on\n",
    "echo \"Compute node: $(hostname)\"\n",
    "\n",
    "# After the job is done we copy our output back to $SLURM_SUBMIT_DIR\n",
    "cp -r ${SCRATCH_DIRECTORY}/results ${SLURM_SUBMIT_DIR}\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"Failed to copy results back to submit directory\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "#### This is how you would clean up the working directory (after copying any important files back!):\n",
    "cd ${SLURM_SUBMIT_DIR}\n",
    "rm -rf ${SCRATCH_DIRECTORY}\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"Failed to clean up scratch directory\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "exit 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This then can be submitted to the cluster using the `sbatch` command.\n",
    "\n",
    "```bash\n",
    "sbatch my_job.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all, now we have upscaled our simulations to a hypercomputing cluster.\n",
    "Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
