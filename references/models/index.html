<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>cpm.models - CPM library</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../assets/_mkdocstrings.css" rel="stylesheet" />
        <link href="../../style.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "cpm.models";
        var mkdocs_page_input_path = "references/models.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/python.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/bash.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/json.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/markdown.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/makefile.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> CPM library
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../about/">About</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">User Guide</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../tutorials/getting-started/">Getting Started</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Examples</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/example1/">Example 1: An associative learning model and blocking</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/example2/">Example 2: Reinforcement learning with a two-armed bandit.</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">API reference</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">cpm.models</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#cpm.models.activation">activation</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.activation.CompetitiveGating">CompetitiveGating</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.activation.CompetitiveGating--references">References</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.activation.CompetitiveGating.compute">compute()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.activation.CompetitiveGating.config">config()</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.activation.Offset">Offset</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.activation.Offset.compute">compute()</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.activation.ProspectUtility">ProspectUtility</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.activation.ProspectUtility--notes">Notes</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.activation.ProspectUtility--references">References</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.activation.ProspectUtility.compute">compute()</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.activation.SigmoidActivation">SigmoidActivation</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.activation.SigmoidActivation.compute">compute()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.activation.SigmoidActivation.config">config()</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#cpm.models.decision">decision</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.decision.ChoiceKernel">ChoiceKernel</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.ChoiceKernel--notes">Notes</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.ChoiceKernel--see-also">See Also</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.ChoiceKernel--references">References</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.decision.GreedyRule">GreedyRule</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.GreedyRule--references">References</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.GreedyRule.choice">choice()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.GreedyRule.compute">compute()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.GreedyRule.config">config()</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.decision.Sigmoid">Sigmoid</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.Sigmoid.choice">choice()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.Sigmoid.compute">compute()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.Sigmoid.config">config()</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.decision.Softmax">Softmax</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.Softmax--notes">Notes</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.Softmax.choice">choice()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.Softmax.compute">compute()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.Softmax.config">config()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.decision.Softmax.irreducible_noise">irreducible_noise()</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#cpm.models.learning">learning</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.learning.DeltaRule">DeltaRule</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.DeltaRule--see-also">See Also</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.DeltaRule--notes">Notes</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.DeltaRule.compute">compute()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.DeltaRule.config">config()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.DeltaRule.reset">reset()</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.learning.HumbleTeacher">HumbleTeacher</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.HumbleTeacher--notes">Notes</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.HumbleTeacher--references">References</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.HumbleTeacher.compute">compute()</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.learning.KernelUpdate">KernelUpdate</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.KernelUpdate--notes">Notes</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.KernelUpdate--see-also">See Also</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.KernelUpdate--references">References</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.KernelUpdate.compute">compute()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.KernelUpdate.config">config()</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.learning.QLearningRule">QLearningRule</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.QLearningRule--notes">Notes</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.QLearningRule--references">References</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.QLearningRule.compute">compute()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.QLearningRule.config">config()</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cpm.models.learning.SeparableRule">SeparableRule</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.SeparableRule--see-also">See Also</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.SeparableRule--notes">Notes</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.SeparableRule--references">References</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.SeparableRule.compute">compute()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.SeparableRule.config">config()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpm.models.learning.SeparableRule.reset">reset()</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#cpm.models.attention">attention</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../generators/">cpm.generators</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../optimisation/">cpm.optimisation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../evaluation/">cpm.evaluation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../utils/">cpm.utils</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">CPM library</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">API reference</li>
      <li class="breadcrumb-item active">cpm.models</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="cpmmodels">cpm.models</h1>


<div class="doc doc-object doc-module">



<h2 id="cpm.models.activation" class="doc doc-heading">
          <code>cpm.models.activation</code>


</h2>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="cpm.models.activation.CompetitiveGating" class="doc doc-heading">
<code class="highlight language-python"><span class="n">CompetitiveGating</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">salience</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">P</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">

  
      <p>A competitive attentional gating function, an attentional activation function, that incorporates stimulus salience in addition to the stimulus vector to modulate the weights.
It formalises the hypothesis that each stimulus has an underlying salience that competes to captures attentional focus (Paskewitz and Jones, 2020; Kruschke, 2001).</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>input</code></td>
          <td>
                <code>array_like</code>
          </td>
          <td><p>The input value. The stimulus representation (vector).</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>values</code></td>
          <td>
                <code>array_like</code>
          </td>
          <td><p>The values. A 2D array of values, where each row represents an outcome and each column represents a single stimulus.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>salience</code></td>
          <td>
                <code>array_like</code>
          </td>
          <td><p>The salience value. A 1D array of salience values, where each value represents the salience of a single stimulus.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>P</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The power value, also called attentional normalisation or brutality, which influences the degree of attentional competition.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>input</code></td>
          <td>
                <code>array_like</code>
          </td>
          <td><p>The input value. The stimulus representation (vector).</p></td>
        </tr>
        <tr>
          <td><code>values</code></td>
          <td>
                <code>array_like</code>
          </td>
          <td><p>The values value. A 2D array of values, where each row represents an outcome and each column represents a single stimulus.</p></td>
        </tr>
        <tr>
          <td><code>salience</code></td>
          <td>
                <code>array_like</code>
          </td>
          <td><p>The salience value. A 1D array of salience values, where each value represents the salience of a single stimulus.</p></td>
        </tr>
        <tr>
          <td><code>P</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The power value, also called attentional normalisation or brutality. It influences the degree of attentional competition.</p></td>
        </tr>
        <tr>
          <td><code>gain</code></td>
          <td>
                <code>array_like</code>
          </td>
          <td><p>The normalised attentional gain for each stimulus, corresponding to the input vector.</p></td>
        </tr>
    </tbody>
  </table>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">salience</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">att</span> <span class="o">=</span> <span class="n">CompetitiveGating</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">salience</span><span class="p">,</span> <span class="n">P</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">att</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([[0.03333333, 0.6       , 0.        ],</span>
<span class="go">       [0.2       , 0.13333333, 0.        ]])</span>
</code></pre></div>
      <h5 id="cpm.models.activation.CompetitiveGating--references">References</h5>
<p>Kruschke, J. K. (2001). Toward a unified model of attention in associative learning. Journal of Mathematical Psychology, 45(6), 812-863.</p>
<p>Paskewitz, S., &amp; Jones, M. (2020). Dissecting exit. Journal of mathematical psychology, 97, 102371.</p>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="cpm.models.activation.CompetitiveGating.compute" class="doc doc-heading">
<code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Compute the activations mediated by underlying salience.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>array_like</code>
          </td>
          <td><p>The values updated with the attentional gain and stimulus vector.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.activation.CompetitiveGating.config" class="doc doc-heading">
<code class="highlight language-python"><span class="n">config</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Print the configuration of the attentional gating function.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>dict</code>
          </td>
          <td><p>The configuration of the attentional gating function.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.activation.Offset" class="doc doc-heading">
<code class="highlight language-python"><span class="n">Offset</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">

  
      <p>A class for adding a scalar to one element of an input array.
In practice, this can be used to "shift" or "offset" the "value" of one particular stimulus, for example to represent a consistent bias for (or against) that stimulus.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>input</code></td>
          <td>
                <code>array_like</code>
          </td>
          <td><p>The input value. The stimulus representation (vector).</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>offset</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The value to be added to one element of the input.</p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
        <tr>
          <td><code>index</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The index of the element of the input vector to which the offset should be added.</p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
                <code>dict, optional</code>
          </td>
          <td><p>Additional keyword arguments.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>input</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The input value. The stimulus representation (vector).</p></td>
        </tr>
        <tr>
          <td><code>offset</code></td>
          <td>
                <code>float or array_like</code>
          </td>
          <td><p>The value(s) to be added to the element(s) of the input. If an array, it should be the same shape as the input, where elements correspond to elements of the input.</p></td>
        </tr>
        <tr>
          <td><code>index</code></td>
          <td>
                <code>int or array_like</code>
          </td>
          <td><p>The index of the element of the input vector to which the offset should be added. If an array, it should be a vector of integers, denoted indices in input.</p></td>
        </tr>
        <tr>
          <td><code>output</code></td>
          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>The stimulus representation (vector) with offset added to the requested element.</p></td>
        </tr>
    </tbody>
  </table>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsetter</span> <span class="o">=</span> <span class="n">Offset</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">vals</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="mf">1.33</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsetter</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([3.43, 1.1])</span>
</code></pre></div>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="cpm.models.activation.Offset.compute" class="doc doc-heading">
<code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Add the offset to the requested input element.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>The stimulus representation (vector) with offset added to the requested element.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.activation.ProspectUtility" class="doc doc-heading">
<code class="highlight language-python"><span class="n">ProspectUtility</span><span class="p">(</span><span class="n">outcomes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">probabilities</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alpha_pos</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha_neg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lambda_loss</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">weighting</span><span class="o">=</span><span class="s1">&#39;tk&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">

  
      <p>A class for computing choice utilities based on prospect theory.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>outcomes</code></td>
          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>The values of potential outcomes for each choice option.
Should be a nested array where the outer dimension represents trials,
followed by options within each trial, followed by potential outcomes within each option.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>probabilities</code></td>
          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>The probabilities of potential outcomes for each choice option.
Should be a nested array where the outer dimension represents trials,
followed by options within each trial, followed by potential outcomes within each option.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>alpha_pos</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The risk attitude parameter for non-negative outcomes, which determines the curvature of the utility function in the gain domain.
If alpha_neg is undefined, alpha_pos will be used for both the gain and loss domains.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>alpha_neg</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The risk attitude parameter for negative outcomes, which determines the curvature of the utility function in the loss domain.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>lambda_loss</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The loss aversion parameter, which scales the utility of negative outcomes relative to non-negative outcomes.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>beta</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The discriminability parameter, which determines the curvature of the weighting function.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>delta</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The attractiveness parameter, which determines the elevation of the weighting function.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>weigting</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>The definition of the weighting function. Should be one of 'tk', 'pd', or 'gw'.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
                <code>dict, optional</code>
          </td>
          <td><p>Additional keyword arguments.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>outcomes</code></td>
          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>The values of potential outcomes for each choice option, for each trial.
Should be a nested array where the outer dimension represents trials,
followed by options within each trial, followed by potential outcomes within each option.</p></td>
        </tr>
        <tr>
          <td><code>probabilities</code></td>
          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>The probabilities of potential outcomes for each choice option, for each trial.
Should be a nested array where the outer dimension represents trials,
followed by options within each trial, followed by potential outcomes within each option.</p></td>
        </tr>
        <tr>
          <td><code>alpha_pos</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The risk attitude parameter for non-negative outcomes, which determines the curvature of the utility function in the gain domain.
If alpha_neg is undefined, alpha_pos will be used for both the gain and loss domains.</p></td>
        </tr>
        <tr>
          <td><code>alpha_neg</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The risk attitude parameter for negative outcomes, which determines the curvature of the utility function in the loss domain.</p></td>
        </tr>
        <tr>
          <td><code>lambda_loss</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The loss aversion parameter, which scales the utility of negative outcomes relative to non-negative outcomes.</p></td>
        </tr>
        <tr>
          <td><code>beta</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The discriminability parameter, which determines the curvature of the weighting function.</p></td>
        </tr>
        <tr>
          <td><code>delta</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The attractiveness parameter, which determines the elevation of the weighting function.</p></td>
        </tr>
        <tr>
          <td><code>weighting</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>The definition of the weighting function. One of 'tk', 'pd', or 'gw'. See Notes for details on each definition.</p></td>
        </tr>
        <tr>
          <td><code>utilities</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The computed utilities of potential outcomes for each choice option, for each trial.</p></td>
        </tr>
        <tr>
          <td><code>weights</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The computed weights of potential outcomes for each choice option, for each trial.</p></td>
        </tr>
        <tr>
          <td><code>expected_utility</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The computed expected utility of each choice option for each trial.</p></td>
        </tr>
    </tbody>
  </table>
      <h5 id="cpm.models.activation.ProspectUtility--notes">Notes</h5>
<p>The different weighting functions currently implemented are:</p>
<pre><code>- `tk`: Tversky &amp; Kahneman (1992).
- `pd`: Prelec (1998).
- `gw`: Gonzalez &amp; Wu (1999).
</code></pre>
<p>Following Tversky &amp; Kahneman (1992), the expected utility U of a choice option is defined as:</p>
<pre><code>U = sum(w(p) * u(x)),
</code></pre>
<p>where w is a weighting function of the probability p of a potential outcome,
and u is the utility function of the value x of a potential outcome.
These functions are defined as follows (equations 6 and 5 respectively in Tversky &amp; Kahneman, 1992, pp. 309):</p>
<pre><code>w(p) = p^beta / (p^beta + (1 - p)^beta)^(1/beta),


u(x) = ifelse(x &gt;= 0, x^alpha_pos, -lambda * (-x)^alpha_neg),
</code></pre>
<p>where beta is the discriminability parameter of the weighting function;
alpha_pos and alpha_neg are the risk attitude parameters in the gain and loss domains respectively,
and lambda is the loss aversion parameter.</p>
<p>Several other definitions of the weighting function have been proposed in the literature,
most notably in Prelec (1998) and Gonzalez &amp; Wu (1999).
Prelec (equation 3.2, 1999, pp. 503) proposed the following definition:</p>
<pre><code>w(p) = exp(-delta * (-log(p))^beta),
</code></pre>
<p>where delta and beta are the attractiveness and discriminability parameters of the weighting function.
Gonzalez &amp; Wu (equation 3, 1999, pp. 139) proposed the following definition:</p>
<pre><code>w(p) = (delta * p^beta) / ((delta * p^beta) + (1-p)^beta).
</code></pre>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">40</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">])],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">])],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prospect</span> <span class="o">=</span> <span class="n">ProspectUtility</span><span class="p">(</span>
<span class="go">        outcomes=vals, probabilities=probs, alpha_pos = 0.85, beta = 0.9</span>
<span class="go">    )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prospect</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([2.44583162, 7.07945784])</span>
</code></pre></div>
      <h5 id="cpm.models.activation.ProspectUtility--references">References</h5>
<p>Gonzalez, R., &amp; Wu, G. (1999). On the shape of the probability weighting function. Cognitive psychology, 38(1), 129-166.</p>
<p>Prelec, D. (1998). The probability weighting function. Econometrica, 497-527.</p>
<p>Tversky, A., &amp; Kahneman, D. (1992). Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and uncertainty, 5, 297-323.</p>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="cpm.models.activation.ProspectUtility.compute" class="doc doc-heading">
<code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Compute the expected utility of each choice option.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>The computed expected utility of each choice option.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.activation.SigmoidActivation" class="doc doc-heading">
<code class="highlight language-python"><span class="n">SigmoidActivation</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">

  
      <p>Represents a sigmoid activation function.</p>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>input</code></td>
          <td>
                <code>array_like</code>
          </td>
          <td><p>The input value. The stimulus representation (vector).
weights : array_like
The weights value. A 2D array of weights, where each row represents an outcome and each column represents a single stimulus.</p></td>
        </tr>
    </tbody>
  </table>

  

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>input</code></td>
          <td>
                <code>array_like</code>
          </td>
          <td><p>The input value. The stimulus representation (vector).</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>weights</code></td>
          <td>
                <code>array_like</code>
          </td>
          <td><p>The weights value. A 2D array of weights, where each row represents an outcome and each column represents a single stimulus.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
                <code>dict</code>
          </td>
          <td><p>Additional keyword arguments.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>


  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="cpm.models.activation.SigmoidActivation.compute" class="doc doc-heading">
<code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Compute the activation value using the sigmoid function.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>kwargs</code></td>
          <td>
                <code>dict</code>
          </td>
          <td><p>Additional keyword arguments.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>The computed activation value.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.activation.SigmoidActivation.config" class="doc doc-heading">
<code class="highlight language-python"><span class="n">config</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Get the configuration of the activation function.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>dict</code>
          </td>
          <td><p>The configuration of the activation function.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="cpm.models.decision" class="doc doc-heading">
          <code>cpm.models.decision</code>


</h2>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="cpm.models.decision.ChoiceKernel" class="doc doc-heading">
<code class="highlight language-python"><span class="n">ChoiceKernel</span><span class="p">(</span><span class="n">temperature_activations</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">temperature_kernel</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">

  
      <p>A class representing a choice kernel based on a softmax function that incorporates the frequency of choosing an action.
It is based on Equation 7 in Wilson and Collins (2019).</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>temperature_activations</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The inverse temperature parameter for the softmax computation.</p></td>
          <td>
                <code>0.5</code>
          </td>
        </tr>
        <tr>
          <td><code>temperature_kernel</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The inverse temperature parameter for the kernel computation.</p></td>
          <td>
                <code>0.5</code>
          </td>
        </tr>
        <tr>
          <td><code>activations</code></td>
          <td>
                <code>ndarray, optional</code>
          </td>
          <td><p>An array of activations for the softmax function.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>kernel</code></td>
          <td>
                <code>ndarray, optional</code>
          </td>
          <td><p>An array of kernel values for the softmax function.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>temperature_a</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The inverse temperature parameter for the softmax computation.</p></td>
        </tr>
        <tr>
          <td><code>temperature_k</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The inverse temperature parameter for the kernel computation.</p></td>
        </tr>
        <tr>
          <td><code>activations</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>An array of activations for the softmax function.</p></td>
        </tr>
        <tr>
          <td><code>kernel</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>An array of kernel values for the softmax function.</p></td>
        </tr>
        <tr>
          <td><code>policies</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>An array of outputs computed using the softmax function.</p></td>
        </tr>
        <tr>
          <td><code>shape</code></td>
          <td>
                <code>tuple</code>
          </td>
          <td><p>The shape of the activations array.</p></td>
        </tr>
    </tbody>
  </table>
      <h5 id="cpm.models.decision.ChoiceKernel--notes">Notes</h5>
<p>In order to get Equation 6 from Wilson and Collins (2019), either set <code>activations</code> to None (default) or set it to 0.</p>
<h5 id="cpm.models.decision.ChoiceKernel--see-also">See Also</h5>
<p><a href="cpm.components.learning.KernelUpdate">cpm.models.learning.KernelUpdate</a>: A class representing a kernel update (Equation 5; Wilson and Collins, 2019) that updates the kernel values.</p>
<h5 id="cpm.models.decision.ChoiceKernel--references">References</h5>
<p>Wilson, R. C., &amp; Collins, A. G. E. (2019). Ten simple rules for the computational modeling of behavioral data. eLife, 8, Article e49547.</p>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">activations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">choice_kernel</span> <span class="o">=</span> <span class="n">ChoiceKernel</span><span class="p">(</span><span class="n">temperature_activations</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">temperature_kernel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">choice_kernel</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([0.44028635, 0.55971365])</span>
</code></pre></div>



  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.decision.GreedyRule" class="doc doc-heading">
<code class="highlight language-python"><span class="n">GreedyRule</span><span class="p">(</span><span class="n">activations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">

  
      <p>A class representing an ε-greedy rule based on Daw et al. (2006).</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>activations</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>An array of activations for the greedy rule.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>epsilon</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Exploration parameter. The probability of selecting a random action.</p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>activations</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>An array of activations for the greedy rule.</p></td>
        </tr>
        <tr>
          <td><code>epsilon</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Exploration parameter. The probability of selecting a random action.</p></td>
        </tr>
        <tr>
          <td><code>policies</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>An array of outputs computed using the greedy rule.</p></td>
        </tr>
        <tr>
          <td><code>shape</code></td>
          <td>
                <code>tuple</code>
          </td>
          <td><p>The shape of the activations array.</p></td>
        </tr>
    </tbody>
  </table>
      <h5 id="cpm.models.decision.GreedyRule--references">References</h5>
<p>Daw, N. D., O’Doherty, J. P., Dayan, P., Seymour, B., &amp; Dolan, R. J. (2006). Cortical substrates for exploratory decisions in humans. Nature, 441(7095), Article 7095. https://doi.org/10.1038/nature04766</p>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="cpm.models.decision.GreedyRule.choice" class="doc doc-heading">
<code class="highlight language-python"><span class="n">choice</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Chooses the action based on the greedy rule.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>action</code></td>          <td>
                <code>int</code>
          </td>
          <td><p>The chosen action based on the greedy rule.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.decision.GreedyRule.compute" class="doc doc-heading">
<code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Computes the greedy rule.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>output</code></td>          <td>
                <code>ndarray</code>
          </td>
          <td><p>A 2D array of outputs computed using the greedy rule.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.decision.GreedyRule.config" class="doc doc-heading">
<code class="highlight language-python"><span class="n">config</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Returns the configuration of the greedy rule.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>config</code></td>          <td>
                <code>dict</code>
          </td>
          <td><p>A dictionary containing the configuration of the greedy rule.</p>
<ul>
<li>activations (ndarray): An array of activations for the greedy rule.</li>
<li>name (str): The name of the greedy rule.</li>
<li>type (str): The class of function it belongs.</li>
</ul></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.decision.Sigmoid" class="doc doc-heading">
<code class="highlight language-python"><span class="n">Sigmoid</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">

  
      <p>A class representing a sigmoid function that takes an n by m array of activations and returns an n
array of outputs, where n is the number of output and m is the number of
inputs.</p>
<pre><code>The sigmoid function is defined as: 1 / (1 + e^(-temperature * (x - beta))).
</code></pre>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>temperature</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The inverse temperature parameter for the sigmoid function.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>beta</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>It is the value of the output activation that results in an output rating
of P = 0.5.</p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
        <tr>
          <td><code>activations</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>An array of activations for the sigmoid function.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>temperature</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The inverse temperature parameter for the sigmoid function.</p></td>
        </tr>
        <tr>
          <td><code>beta</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The bias parameter for the sigmoid function. It is the value of the
output activation that results in an output rating of P = 0.5.</p></td>
        </tr>
        <tr>
          <td><code>activations</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>An array of activations for the sigmoid function.</p></td>
        </tr>
        <tr>
          <td><code>policies</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>An array of outputs computed using the sigmoid function.</p></td>
        </tr>
        <tr>
          <td><code>shape</code></td>
          <td>
                <code>tuple</code>
          </td>
          <td><p>The shape of the activations array.</p></td>
        </tr>
    </tbody>
  </table>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="cpm.models.decision.Sigmoid.choice" class="doc doc-heading">
<code class="highlight language-python"><span class="n">choice</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Chooses the action based on the sigmoid function.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>action</code></td>          <td>
                <code>int</code>
          </td>
          <td><p>The chosen action based on the sigmoid function.</p></td>
        </tr>
    </tbody>
  </table>
      <h6 id="cpm.models.decision.Sigmoid.choice--notes">Notes</h6>
<p>The choice is based on the probabilities of the sigmoid function, but it is not
guaranteed that the policy values will sum to 1. Therefore, the policies
are normalised to sum to 1 when generating a discrete choice.</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.decision.Sigmoid.compute" class="doc doc-heading">
<code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Computes the Sigmoid function.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>output</code></td>          <td>
                <code>ndarray</code>
          </td>
          <td><p>A 2D array of outputs computed using the sigmoid function.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.decision.Sigmoid.config" class="doc doc-heading">
<code class="highlight language-python"><span class="n">config</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Returns the configuration of the sigmoid function.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>config</code></td>          <td>
                <code>dict</code>
          </td>
          <td><p>A dictionary containing the configuration of the sigmoid function.</p>
<ul>
<li>temperature (float): The temperature parameter for the sigmoid function.</li>
<li>beta (float): The bias parameter for the sigmoid function.</li>
<li>activations (ndarray): An array of activations for the sigmoid function.</li>
<li>name (str): The name of the sigmoid function.</li>
<li>type (str): The class of function it belongs.</li>
</ul></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.decision.Softmax" class="doc doc-heading">
<code class="highlight language-python"><span class="n">Softmax</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">

  
      <p>Softmax class for computing policies based on activations and temperature.</p>
<pre><code>The softmax function is defined as: e^(temperature * x) / sum(e^(temperature * x)).
</code></pre>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>temperature</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The inverse temperature parameter for the softmax computation.</p></td>
        </tr>
        <tr>
          <td><code>xi</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The irreducible noise parameter for the softmax computation.</p></td>
        </tr>
        <tr>
          <td><code>activations</code></td>
          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>Array of activations for each possible outcome/action. It should be
a 2D ndarray, where each row represents an outcome and each column
represents a single stimulus.</p></td>
        </tr>
        <tr>
          <td><code>policies</code></td>
          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>Array of computed policies.</p></td>
        </tr>
        <tr>
          <td><code>shape</code></td>
          <td>
                <code>tuple</code>
          </td>
          <td><p>The shape of the activations array.</p></td>
        </tr>
    </tbody>
  </table>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>temperature</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The inverse temperature parameter for the softmax computation.</p></td>
          <td>
                <code>0.5</code>
          </td>
        </tr>
        <tr>
          <td><code>xi</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The irreducible noise parameter for the softmax computation.</p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
        <tr>
          <td><code>activations</code></td>
          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>Array of activations for each possible outcome/action. It should be
a 2D ndarray, where each row represents an outcome and each column
represents a single stimulus.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>
      <h5 id="cpm.models.decision.Softmax--notes">Notes</h5>
<p>The inverse temperature parameter beta represents the degree of randomness in the choice process.
As beta approaches positive infinity, choices becomes more deterministic,
such that the choice option with the greatest activation is more likely to be chosen - it approximates a step function.
By contrast, as beta approaches zero, choices becomes random (i.e., the probabilities the choice options are approximately equal)
and therefore independent of the options' activations.</p>
<p>Note that if you have one value for each outcome (i.e. a classical bandit-like problem), and you represent it as a 1D
array, you must reshape it in the format specified for activations. So that if you have 3 stimuli
which all are actionable, <code>[0.1, 0.5, 0.22]</code>, you should have a 2D array of shape (3, 1), <code>[[0.1], [0.5], [0.22]]</code>.
You can see <a href="" title="./examples/examples2">Example 2</a> for a demonstration.</p>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cpm.components.decision</span> <span class="kn">import</span> <span class="n">Softmax</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">temperature</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">activations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmax</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([0.45352133, 0.54647867])</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">softmax</span><span class="o">.</span><span class="n">config</span><span class="p">()</span>
<span class="go">{</span>
<span class="go">    &quot;temperature&quot;   : 1,</span>
<span class="go">    &quot;activations&quot;:</span>
<span class="go">        array([[ 0.1,  0. ,  0.2],</span>
<span class="go">        [-0.6,  0. ,  0.9]]),</span>
<span class="go">    &quot;name&quot;  : &quot;Softmax&quot;,</span>
<span class="go">    &quot;type&quot;  : &quot;decision&quot;,</span>
<span class="go">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Softmax</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">)</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([0.45352133, 0.54647867])</span>
</code></pre></div>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="cpm.models.decision.Softmax.choice" class="doc doc-heading">
<code class="highlight language-python"><span class="n">choice</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Choose an action based on the computed policies.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>int</code></td>          <td>
                <code>The chosen action based on the computed policies.</code>
          </td>
          <td></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.decision.Softmax.compute" class="doc doc-heading">
<code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Compute the policies based on the activations and temperature.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.decision.Softmax.config" class="doc doc-heading">
<code class="highlight language-python"><span class="n">config</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Get the configuration of the Softmax class.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="cpm.models.decision.Softmax.config" href="#cpm.models.decision.Softmax.config">config</a>(dict)</code>
          </td>
          <td></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.decision.Softmax.irreducible_noise" class="doc doc-heading">
<code class="highlight language-python"><span class="n">irreducible_noise</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Extended softmax class for computing policies based on activations, with parameters inverse temperature and irreducible noise.</p>
<p>The softmax function with irreducible noise is defined as:</p>
<pre><code>(e^(beta * x) / sum(e^(beta * x))) * (1 - xi) + (xi / length(x)),
</code></pre>
<p>where x is the input array of activations, beta is the inverse temperature parameter, and xi is the irreducible noise parameter.</p>
<h6 id="cpm.models.decision.Softmax.irreducible_noise--notes">Notes</h6>
<p>The irreducible noise parameter xi accounts for attentional lapses in the choice process.
Specifically, the terms (1-xi) + (xi/length(x)) cause the choice probabilities to be proportionally scaled towards 1/length(x).
Relatively speaking, this increases the probability that an option is selected if its activation is exceptionally low.
This may seem counterintuitive in theory, but in practice it enables the model to capture highly surprising responses that can occur during attentional lapses.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td></td>
        </tr>
    </tbody>
  </table>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">activations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">noisy_softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">noisy_softmax</span><span class="o">.</span><span class="n">irreducible_noise</span><span class="p">()</span>
<span class="go">array([0.4101454, 0.5898546])</span>
</code></pre></div>

  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="cpm.models.learning" class="doc doc-heading">
          <code>cpm.models.learning</code>


</h2>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="cpm.models.learning.DeltaRule" class="doc doc-heading">
<code class="highlight language-python"><span class="n">DeltaRule</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">

  
      <p>DeltaRule class computes the prediction error for a given input and target value.
It is based on the Gluck and Bower's (1988) delta rule, an extension to Rescorla
and Wagner (1972), which was identical to that of Widrow and Hoff (1960).</p>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>alpha</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The learning rate.</p></td>
        </tr>
        <tr>
          <td><code>input</code></td>
          <td>
                <code>ndarray or array_like</code>
          </td>
          <td><p>The input value. The stimulus representation in the form of a 1D array, where each element can take a value of 0 and 1.</p></td>
        </tr>
        <tr>
          <td><code>weights</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The weights value. A 2D array of weights, where each row represents an outcome and each column represents a single stimulus.</p></td>
        </tr>
        <tr>
          <td><code>teacher</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The target values or feedback, sometimes referred to as teaching signals. These are the values that the algorithm should learn to predict.</p></td>
        </tr>
        <tr>
          <td><code>shape</code></td>
          <td>
                <code>tuple</code>
          </td>
          <td><p>The shape of the weight matrix.</p></td>
        </tr>
    </tbody>
  </table>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>alpha</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The learning rate.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>weights</code></td>
          <td>
                <code>array-like</code>
          </td>
          <td><p>The input value. The stimulus representation in the form of a 1D array, where each element can take a value of 0 and 1.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>feedback</code></td>
          <td>
                <code>array-like</code>
          </td>
          <td><p>The target values or feedback, sometimes referred to as teaching signals. These are the values that the algorithm should learn to predict.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>input</code></td>
          <td>
                <code>array-like</code>
          </td>
          <td><p>The input value. The stimulus representation in the form of a 1D array, where each element can take a value of 0 and 1.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
                <code>dict, optional</code>
          </td>
          <td><p>Additional keyword arguments.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>
      <h5 id="cpm.models.learning.DeltaRule--see-also">See Also</h5>
<p><a class="autorefs autorefs-internal" href="#cpm.models.learning.SeparableRule">cpm.models.learning.SeparableRule</a> : A class representing a learning rule based on the separable error-term of Bush and Mosteller (1951).</p>
<h5 id="cpm.models.learning.DeltaRule--notes">Notes</h5>
<p>The delta-rule is a summed error term, which means that the error is defined as
the difference between the target value and the summed activation of all values
for a given output unit available on the current trial/state. For separable
error term, see the Bush and Mosteller (1951) rule.</p>
<p>The current implementation is based on the Gluck and Bower's (1988) delta rule, an
extension of the Rescorla and Wagner (1972) learning rule to multi-outcome learning.</p>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cpm.components.learning</span> <span class="kn">import</span> <span class="n">DeltaRule</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">teacher</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">delta_rule</span> <span class="o">=</span> <span class="n">DeltaRule</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="n">teacher</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">delta_rule</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([[ 0.04,  0.04,  0.  ],</span>
<span class="go">       [-0.15, -0.15, -0.  ]])</span>
</code></pre></div>
    <p>This implementation generalises to n-dimensional weight matrices, which means
that it can be applied to both single- and multi-outcome learning paradigms.</p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">teacher</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">delta_rule</span> <span class="o">=</span> <span class="n">DeltaRule</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="n">teacher</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">delta_rule</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([[0.03, 0.03, 0.  , 0.  ]])</span>
</code></pre></div>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="cpm.models.learning.DeltaRule.compute" class="doc doc-heading">
<code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Compute the weights using the CPM learning rule.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>weights</code></td>          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>The updated weights matrix.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.learning.DeltaRule.config" class="doc doc-heading">
<code class="highlight language-python"><span class="n">config</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Get the configuration of the learning component.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>config</code></td>          <td>
                <code>dict</code>
          </td>
          <td><p>A dictionary containing the configuration parameters of the learning component.</p>
<ul>
<li>alpha (float): The learning rate.</li>
<li>weights (list): The weights used for learning.</li>
<li>teacher (str): The name of the teacher.</li>
<li>name (str): The name of the learning component class.</li>
<li>type (str): The type of the learning component.</li>
</ul></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.learning.DeltaRule.reset" class="doc doc-heading">
<code class="highlight language-python"><span class="n">reset</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Reset the weights to zero.</p>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.learning.HumbleTeacher" class="doc doc-heading">
<code class="highlight language-python"><span class="n">HumbleTeacher</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">

  
      <p>A humbe teacher learning rule (Kruschke, 1992; Love, Gureckis, and Medin, 2004) for multi-dimensional outcome learning.</p>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>alpha</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The learning rate.</p></td>
        </tr>
        <tr>
          <td><code>input</code></td>
          <td>
                <code>ndarray or array_like</code>
          </td>
          <td><p>The input value. The stimulus representation in the form of a 1D array, where each element can take a value of 0 and 1.</p></td>
        </tr>
        <tr>
          <td><code>weights</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The weights value. A 2D array of weights, where each row represents an outcome and each column represents a single stimulus.</p></td>
        </tr>
        <tr>
          <td><code>teacher</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The target values or feedback, sometimes referred to as teaching signals. These are the values that the algorithm should learn to predict.</p></td>
        </tr>
        <tr>
          <td><code>shape</code></td>
          <td>
                <code>tuple</code>
          </td>
          <td><p>The shape of the weight matrix.</p></td>
        </tr>
    </tbody>
  </table>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>alpha</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The learning rate.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>weights</code></td>
          <td>
                <code>array-like</code>
          </td>
          <td><p>The input value. The stimulus representation in the form of a 1D array, where each element can take a value of 0 and 1.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>feedback</code></td>
          <td>
                <code>array-like</code>
          </td>
          <td><p>The target values or feedback, sometimes referred to as teaching signals. These are the values that the algorithm should learn to predict.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>input</code></td>
          <td>
                <code>array-like</code>
          </td>
          <td><p>The input value. The stimulus representation in the form of a 1D array, where each element can take a value of 0 and 1.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
                <code>dict, optional</code>
          </td>
          <td><p>Additional keyword arguments.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>
      <h5 id="cpm.models.learning.HumbleTeacher--notes">Notes</h5>
<p>The humble teacher learning rule is a learning rule that is based on the idea that if output node activations large than the teaching signals should not be counted as error, but should be rewarded. So the humble teacher turns teaching signals into discrete (nominal) values, where they do not indicate the degree of membership between stimuli and outcome label, the degree of causality between stimuli and outcome, or the degree of correctness of the output.</p>
<h5 id="cpm.models.learning.HumbleTeacher--references">References</h5>
<p>Kruschke, J. K. (1992). ALCOVE: An exemplar-based connectionist model of category learning. Psychological Review, 99, 22–44.</p>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cpm.components.learning</span> <span class="kn">import</span> <span class="n">HumbleTeacher</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">teacher</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">humble_teacher</span> <span class="o">=</span> <span class="n">HumbleTeacher</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="n">teacher</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">humble_teacher</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([[-0.06, -0.06, -0.06],</span>
<span class="go">   [ 0.  ,  0.  ,  0.  ]])</span>
</code></pre></div>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="cpm.models.learning.HumbleTeacher.compute" class="doc doc-heading">
<code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Compute the weights using the CPM learning rule.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>weights</code></td>          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>The updated weights matrix.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.learning.KernelUpdate" class="doc doc-heading">
<code class="highlight language-python"><span class="n">KernelUpdate</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">

  
      <p>A class representing a learning rule for updating the choice kernel as specified by Equation 5 in Wilson and Collins (2019).</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>response</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The response vector. It must be a binary numpy.ndarray, so that each element corresponds to a response option. If there are 4 response options, and the second was selected, it would be represented as <code>[0, 1, 0, 0]</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>alpha</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The kernel learning rate.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>kernel</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The kernel used for learning. It is a 1D array of kernel values, where each element corresponds to a response option. Each element must correspond to the same response option in the <code>response</code> vector.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>response</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The response vector. It must be a binary numpy.ndarray, so that each element corresponds to a response option. If there are 4 response options, and the second was selected, it would be represented as <code>[0, 1, 0, 0]</code>.</p></td>
        </tr>
        <tr>
          <td><code>alpha</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The kernel learning rate.</p></td>
        </tr>
        <tr>
          <td><code>kernel</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The kernel used for learning. It is a 1D array of kernel values, where each element corresponds to a response option. Each element must correspond to the same response option in the <code>response</code> vector.</p></td>
        </tr>
    </tbody>
  </table>
      <h5 id="cpm.models.learning.KernelUpdate--notes">Notes</h5>
<p>The kernel update component is used to represent how likely a given response is to be chosen based on the frequency it was chosen in the past.
This can then be integrated into a choice kernel decision policy.</p>
<h5 id="cpm.models.learning.KernelUpdate--see-also">See Also</h5>
<p><a class="autorefs autorefs-internal" href="#cpm.models.decision.ChoiceKernel">cpm.models.decision.ChoiceKernel</a> : A class representing a choice kernel decision policy.</p>
<h5 id="cpm.models.learning.KernelUpdate--references">References</h5>
<p>Wilson, Robert C., and Anne GE Collins. Ten simple rules for the computational modeling of behavioral data. Elife 8 (2019): e49547.</p>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="cpm.models.learning.KernelUpdate.compute" class="doc doc-heading">
<code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Compute the change in the kernel based on the given response, rate, and kernel, and return the updated kernel.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>output</code></td>          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>The computed change of the kernel.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.learning.KernelUpdate.config" class="doc doc-heading">
<code class="highlight language-python"><span class="n">config</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Get the configuration of the kernel update component.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>config</code></td>          <td>
                <code>dict</code>
          </td>
          <td><p>A dictionary containing the configuration parameters of the kernel update component.</p>
<ul>
<li>response (float): The response of the system.</li>
<li>rate (float): The learning rate.</li>
<li>kernel (list): The kernel used for learning.</li>
<li>input (str): The name of the input.</li>
<li>name (str): The name of the kernel update component class.</li>
<li>type (str): The type of the kernel update component.</li>
</ul></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.learning.QLearningRule" class="doc doc-heading">
<code class="highlight language-python"><span class="n">QLearningRule</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reward</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">

  
      <p>Q-learning rule (Watkins, 1989) for a one-dimensional array of Q-values.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>alpha</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The learning rate. Default is 0.5.</p></td>
          <td>
                <code>0.5</code>
          </td>
        </tr>
        <tr>
          <td><code>gamma</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The discount factor. Default is 0.1.</p></td>
          <td>
                <code>0.1</code>
          </td>
        </tr>
        <tr>
          <td><code>values</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The values matrix.  It is a 1D array of Q-values active for the current state, where each element corresponds to an action.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>reward</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The reward received on the current state.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>maximum</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The maximum estimated reward for the next state.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>alpha</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The learning rate.</p></td>
        </tr>
        <tr>
          <td><code>gamma</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The discount factor.</p></td>
        </tr>
        <tr>
          <td><code>values</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The values matrix. It is a 1D array of Q-values, where each element corresponds to an action.</p></td>
        </tr>
        <tr>
          <td><code>reward</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The reward received on the current state.</p></td>
        </tr>
        <tr>
          <td><code>maximum</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The maximum estimated reward for the next state.</p></td>
        </tr>
    </tbody>
  </table>
      <h5 id="cpm.models.learning.QLearningRule--notes">Notes</h5>
<p>The Q-learning rule is a model-free reinforcement learning algorithm that is used to learn the value of an action in a given state.
It is defined as</p>
<pre><code>Q(s, a) = Q(s, a) + alpha * (r + gamma * max(Q(s', a')) - Q(s, a)),
</code></pre>
<p>where <code>Q(s, a)</code> is the value of action <code>a</code> in state <code>s</code>, <code>r</code> is the reward received on the current state, <code>gamma</code> is the discount factor, and <code>max(Q(s', a'))</code> is the maximum estimated reward for the next state.</p>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cpm.models.learning</span> <span class="kn">import</span> <span class="n">QLearningRule</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">component</span> <span class="o">=</span> <span class="n">QLearningRule</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">,</span> <span class="n">reward</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">component</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="go">array([1.8  , 1.35 , 1.791])</span>
</code></pre></div>
      <h5 id="cpm.models.learning.QLearningRule--references">References</h5>
<p>Watkins, C. J. C. H. (1989). Learning from delayed rewards.</p>
<p>Watkins, C. J., &amp; Dayan, P. (1992). Q-learning. Machine learning, 8, 279-292.</p>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="cpm.models.learning.QLearningRule.compute" class="doc doc-heading">
<code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Compute the change in values based on the given values, reward, and parameters, and return the updated values.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>output</code></td>          <td>
                <code>numpy.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>The computed output values.</p></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.learning.QLearningRule.config" class="doc doc-heading">
<code class="highlight language-python"><span class="n">config</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Get the configuration of the q-learning component.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>config</code></td>          <td>
                <code>dict</code>
          </td>
          <td><p>A dictionary containing the configuration parameters of the learning component.</p>
<ul>
<li>alpha (float): The learning rate.</li>
<li>gamma (float): The discount factor.</li>
<li>values (list): The values used for learning.</li>
<li>reward (str): The name of the reward.</li>
<li>maximum (str): The name of the maximum reward.</li>
<li>name (str): The name of the learning component class.</li>
<li>type (str): The type of the learning component.</li>
</ul></td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="cpm.models.learning.SeparableRule" class="doc doc-heading">
<code class="highlight language-python"><span class="n">SeparableRule</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">

  
      <p>A class representing a learning rule based on the separable error-term of
Bush and Mosteller (1951).</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>alpha</code></td>
          <td>
                <code>float, optional</code>
          </td>
          <td><p>The learning rate.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>weights</code></td>
          <td>
                <code>array-like, optional</code>
          </td>
          <td><p>The input value. The stimulus representation in the form of a 1D array, where each element can take a value of 0 and 1.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>feedback</code></td>
          <td>
                <code>array-like, optional</code>
          </td>
          <td><p>The target values or feedback, sometimes referred to as teaching signals. These are the values that the algorithm should learn to predict.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>input</code></td>
          <td>
                <code>array-like, optional</code>
          </td>
          <td><p>The input value. The stimulus representation in the form of a 1D array, where each element can take a value of 0 and 1.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
                <code>dict, optional</code>
          </td>
          <td><p>Additional keyword arguments.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Attributes:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>alpha</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The learning rate.</p></td>
        </tr>
        <tr>
          <td><code>input</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The input value. The stimulus representation in the form of a 1D array, where each element can take a value of 0 and 1.</p></td>
        </tr>
        <tr>
          <td><code>weights</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The weights value. A 2D array of weights, where each row represents an outcome and each column represents a single stimulus.</p></td>
        </tr>
        <tr>
          <td><code>teacher</code></td>
          <td>
                <code>ndarray</code>
          </td>
          <td><p>The target values or feedback, sometimes referred to as teaching signals. These are the values that the algorithm should learn to predict.</p></td>
        </tr>
        <tr>
          <td><code>shape</code></td>
          <td>
                <code>tuple</code>
          </td>
          <td><p>The shape of the weights array.</p></td>
        </tr>
    </tbody>
  </table>
      <h5 id="cpm.models.learning.SeparableRule--see-also">See Also</h5>
<p><a class="autorefs autorefs-internal" href="#cpm.models.learning.DeltaRule">cpm.models.learning.DeltaRule</a> : An extension of the Rescorla and Wagner (1972) learning rule by Gluck and Bower (1988) to allow multi-outcome learning.</p>
<h5 id="cpm.models.learning.SeparableRule--notes">Notes</h5>
<p>This type of learning rule was among the earliest formal models of associative learning (Le Pelley, 2004), which were based on standard linear operators (Bush &amp; Mosteller, 1951; Estes, 1950; Kendler, 1971).</p>
<h5 id="cpm.models.learning.SeparableRule--references">References</h5>
<p>Bush, R. R., &amp; Mosteller, F. (1951). A mathematical model for simple learning. Psychological Review, 58, 313–323</p>
<p>Estes, W. K. (1950). Toward a statistical theory of learning. Psychological Review, 57, 94–107</p>
<p>Kendler, T. S. (1971). Continuity theory and cue dominance. In J. T. Spence (Ed.), Essays in neobehaviorism: A memorial volume to Kenneth W. Spence. New York: Appleton-Century-Crofts.</p>
<p>Le Pelley, M. E. (2004). The role of associative history in models of associative learning: A selective review and a hybrid model. Quarterly Journal of Experimental Psychology Section B, 57(3), 193-243.</p>



  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="cpm.models.learning.SeparableRule.compute" class="doc doc-heading">
<code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Computes the updated weights using the learning rule.</p>
<h6 id="cpm.models.learning.SeparableRule.compute--returns">Returns:</h6>
<p>ndarray
    The updated weights.</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.learning.SeparableRule.config" class="doc doc-heading">
<code class="highlight language-python"><span class="n">config</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Returns a dictionary containing the configuration of the object.</p>
<h6 id="cpm.models.learning.SeparableRule.config--returns">Returns:</h6>
<p>dict
    The configuration of the object.</p>

  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="cpm.models.learning.SeparableRule.reset" class="doc doc-heading">
<code class="highlight language-python"><span class="n">reset</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Resets the weights to zero.</p>

  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="cpm.models.attention" class="doc doc-heading">
          <code>cpm.models.attention</code>


</h2>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">











  </div>

  </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../examples/example2/" class="btn btn-neutral float-left" title="Example 2: Reinforcement learning with a two-armed bandit."><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../generators/" class="btn btn-neutral float-right" title="cpm.generators">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../examples/example2/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../generators/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
